aws
all;;;#total = 3888; 
 acc = 65.69%; f1_male = 71.57% (p: 59.85 / r: 88.99); f1_female = 62.75% (p: 81.56 / r: 50.99)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 69.83%, female: 29.3%, neutral: 0.0%, unknown: 0.87%
male professions = ['taxpayer', 'pedestrian', 'inspector', 'therapist', 'onlooker', 'specialist', 'electrician', 'officer', 'pathologist', 'plumber', 'surgeon', 'examiner', 'buyer', 'paralegal', 'dispatcher', 'bystander', 'doctor', 'firefighter']
female professions = ['child', 'witness', 'bartender', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'employee', 'engineer', 'client', 'worker', 'student', 'educator', 'patient', 'homeowner', 'teenager', 'undergraduate', 'administrator', 'visitor', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'investigator', 'protester', 'resident', 'planner', 'practitioner', 'instructor', 'owner', 'veterinarian', 'paramedic', 'passenger', 'chemist', 'machinist', 'appraiser', 'nutritionist', 'architect', 'programmer', 'hygienist', 'scientist', 'dietitian', 'painter', 'broker', 'guest', 'chef']
Unknown male: 13
Unknown female: 19
Unknown neutral: 2
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fa945b83670>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa945b838b0>,
                                           {<GENDER.male: 0>: 1625,
                                            <GENDER.female: 1>: 188,
                                            <GENDER.unknown: 3>: 13}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa945b839d0>,
                                             {<GENDER.male: 0>: 874,
                                              <GENDER.female: 1>: 929,
                                              <GENDER.unknown: 3>: 19}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa945b83af0>,
                                              {<GENDER.male: 0>: 216,
                                               <GENDER.female: 1>: 22,
                                               <GENDER.unknown: 3>: 2})})
{"acc": 65.7, "f1_male": 71.6, "f1_female": 62.8, "unk_male": 13, "unk_female": 19, "unk_neutral": 2}
pro-stereotypical;;;#total = 1584; 
 acc = 81.12%; f1_male = 84.33% (p: 73.88 / r: 98.23); f1_female = 77.41% (p: 97.88 / r: 64.02)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 66.48%, female: 32.7%, neutral: 0.0%, unknown: 0.82%
male professions = ['developer', 'mechanic', 'analyst', 'chief', 'salesperson', 'lawyer', 'physician', 'farmer', 'manager', 'laborer', 'construction worker', 'carpenter', 'janitor', 'supervisor', 'sheriff']
female professions = ['nurse', 'cleaner']
neutral professions = []
ambiguous professions = ['designer', 'clerk', 'mover', 'housekeeper', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'ceo', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cashier', 'tailor', 'writer', 'counselor', 'accountant', 'attendant']
Unknown male: 3
Unknown female: 10
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fe6e3389f70>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe6eefeb040>,
                                           {<GENDER.male: 0>: 778,
                                            <GENDER.female: 1>: 11,
                                            <GENDER.unknown: 3>: 3}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe6eefebb80>,
                                             {<GENDER.male: 0>: 275,
                                              <GENDER.female: 1>: 507,
                                              <GENDER.unknown: 3>: 10})})
{"acc": 81.1, "f1_male": 84.3, "f1_female": 77.4, "unk_male": 3, "unk_female": 10, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 58.96%; f1_male = 65.89% (p: 56.86 / r: 78.34); f1_female = 49.29% (p: 65.55 / r: 39.49)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 69.07%, female: 30.05%, neutral: 0.0%, unknown: 0.88%
male professions = ['clerk', 'assistant', 'librarian', 'hairdresser', 'teacher', 'editor', 'writer', 'accountant', 'attendant']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'mover', 'housekeeper', 'analyst', 'chief', 'salesperson', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'construction worker', 'counselor', 'carpenter', 'janitor', 'supervisor', 'sheriff']
Unknown male: 8
Unknown female: 6
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fe8ee38a160>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe8ee38a0d0>,
                                           {<GENDER.male: 0>: 622,
                                            <GENDER.female: 1>: 164,
                                            <GENDER.unknown: 3>: 8}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe8ee38a9d0>,
                                             {<GENDER.male: 0>: 472,
                                              <GENDER.female: 1>: 312,
                                              <GENDER.unknown: 3>: 6})})
{"acc": 59.0, "f1_male": 65.9, "f1_female": 49.3, "unk_male": 8, "unk_female": 6, "unk_neutral": 0}
bing
all;;;#total = 3888; 
 acc = 57.69%; f1_male = 67.51% (p: 53.83 / r: 90.53); f1_female = 45.57% (p: 76.92 / r: 32.38)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 78.99%, female: 19.73%, neutral: 0.0%, unknown: 1.29%
male professions = ['physician', 'janitor', 'sheriff', 'technician', 'taxpayer', 'employee', 'pedestrian', 'inspector', 'therapist', 'visitor', 'pharmacist', 'onlooker', 'investigator', 'specialist', 'electrician', 'officer', 'protester', 'pathologist', 'plumber', 'surgeon', 'owner', 'veterinarian', 'paramedic', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'scientist', 'dispatcher', 'bystander', 'painter', 'guest', 'doctor', 'firefighter']
female professions = ['child', 'advisee', 'witness', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'accountant', 'supervisor', 'attendant', 'customer', 'someone', 'engineer', 'client', 'worker', 'student', 'educator', 'patient', 'homeowner', 'teenager', 'undergraduate', 'administrator', 'advisor', 'psychologist', 'bartender', 'resident', 'planner', 'practitioner', 'instructor', 'passenger', 'nutritionist', 'architect', 'programmer', 'paralegal', 'hygienist', 'dietitian', 'broker', 'chef']
Unknown male: 14
Unknown female: 35
Unknown neutral: 1
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f9201526700>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9201526a60>,
                                           {<GENDER.male: 0>: 1653,
                                            <GENDER.female: 1>: 159,
                                            <GENDER.unknown: 3>: 14}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9201526ca0>,
                                             {<GENDER.male: 0>: 1197,
                                              <GENDER.female: 1>: 590,
                                              <GENDER.unknown: 3>: 35}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9201526c10>,
                                              {<GENDER.male: 0>: 221,
                                               <GENDER.female: 1>: 18,
                                               <GENDER.unknown: 3>: 1})})
{"acc": 57.7, "f1_male": 67.5, "f1_female": 45.6, "unk_male": 14, "unk_female": 35, "unk_neutral": 1}
pro-stereotypical;;;#total = 1584; 
 acc = 72.1%; f1_male = 78.42% (p: 65.37 / r: 97.98); f1_female = 62.67% (p: 97.34 / r: 46.21)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 74.94%, female: 23.74%, neutral: 0.0%, unknown: 1.33%
male professions = ['developer', 'analyst', 'chief', 'salesperson', 'lawyer', 'physician', 'farmer', 'manager', 'laborer', 'carpenter', 'janitor', 'supervisor', 'sheriff']
female professions = ['secretary']
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'cleaner', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'accountant', 'attendant']
Unknown male: 6
Unknown female: 15
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fe27fcb5f70>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe27fcb9f70>,
                                           {<GENDER.male: 0>: 776,
                                            <GENDER.female: 1>: 10,
                                            <GENDER.unknown: 3>: 6}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe27fcb9310>,
                                             {<GENDER.male: 0>: 411,
                                              <GENDER.female: 1>: 366,
                                              <GENDER.unknown: 3>: 15})})
{"acc": 72.1, "f1_male": 78.4, "f1_female": 62.7, "unk_male": 6, "unk_female": 15, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 50.13%; f1_male = 62.55% (p: 50.75 / r: 81.49); f1_female = 27.33% (p: 51.4 / r: 18.61)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 80.49%, female: 18.06%, neutral: 0.0%, unknown: 1.45%
male professions = ['assistant', 'librarian', 'hairdresser', 'teacher', 'physician', 'writer', 'counselor', 'carpenter', 'janitor', 'accountant', 'attendant', 'sheriff']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'chief', 'salesperson', 'lawyer', 'cook', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'construction worker', 'supervisor']
Unknown male: 8
Unknown female: 15
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fc4f691cc10>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fc4f691c820>,
                                           {<GENDER.male: 0>: 647,
                                            <GENDER.female: 1>: 139,
                                            <GENDER.unknown: 3>: 8}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fc4f691ce50>,
                                             {<GENDER.male: 0>: 628,
                                              <GENDER.female: 1>: 147,
                                              <GENDER.unknown: 3>: 15})})
{"acc": 50.1, "f1_male": 62.5, "f1_female": 27.3, "unk_male": 8, "unk_female": 15, "unk_neutral": 0}
google
all;;;#total = 3888; 
 acc = 50.98%; f1_male = 63.75% (p: 49.6 / r: 89.21); f1_female = 29.72% (p: 63.83 / r: 19.37)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 84.47%, female: 14.22%, neutral: 0.0%, unknown: 1.31%
male professions = ['salesperson', 'manager', 'laborer', 'construction worker', 'janitor', 'accountant', 'sheriff', 'customer', 'technician', 'taxpayer', 'employee', 'engineer', 'client', 'pedestrian', 'homeowner', 'inspector', 'therapist', 'administrator', 'visitor', 'advisee', 'advisor', 'pharmacist', 'onlooker', 'investigator', 'specialist', 'electrician', 'officer', 'protester', 'pathologist', 'planner', 'practitioner', 'plumber', 'instructor', 'surgeon', 'owner', 'veterinarian', 'paramedic', 'passenger', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'architect', 'programmer', 'paralegal', 'scientist', 'dispatcher', 'bystander', 'dietitian', 'painter', 'broker', 'guest', 'chef', 'doctor', 'firefighter']
female professions = ['undergraduate', 'child', 'witness', 'bartender', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'counselor', 'carpenter', 'supervisor', 'attendant', 'someone', 'worker', 'student', 'educator', 'patient', 'teenager', 'psychologist', 'resident', 'nutritionist', 'hygienist']
Unknown male: 18
Unknown female: 30
Unknown neutral: 3
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7faa02d3edc0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7faa02cc43a0>,
                                           {<GENDER.male: 0>: 1629,
                                            <GENDER.female: 1>: 179,
                                            <GENDER.unknown: 3>: 18}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7faa02cc4430>,
                                             {<GENDER.male: 0>: 1439,
                                              <GENDER.female: 1>: 353,
                                              <GENDER.unknown: 3>: 30}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7faa02d3e3a0>,
                                              {<GENDER.male: 0>: 216,
                                               <GENDER.female: 1>: 21,
                                               <GENDER.unknown: 3>: 3})})
{"acc": 51.0, "f1_male": 63.8, "f1_female": 29.8, "unk_male": 18, "unk_female": 30, "unk_neutral": 3}
pro-stereotypical;;;#total = 1584; 
 acc = 65.34%; f1_male = 74.37% (p: 59.98 / r: 97.85); f1_female = 48.92% (p: 95.94 / r: 32.83)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 81.57%, female: 17.11%, neutral: 0.0%, unknown: 1.33%
male professions = ['developer', 'mechanic', 'mover', 'analyst', 'chief', 'salesperson', 'lawyer', 'physician', 'manager', 'laborer', 'construction worker', 'carpenter', 'janitor', 'accountant', 'supervisor', 'sheriff']
female professions = ['secretary']
neutral professions = []
ambiguous professions = ['designer', 'clerk', 'housekeeper', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'cleaner', 'cashier', 'tailor', 'writer', 'counselor', 'attendant']
Unknown male: 6
Unknown female: 15
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fd60f37da60>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd60f37d700>,
                                           {<GENDER.male: 0>: 775,
                                            <GENDER.female: 1>: 11,
                                            <GENDER.unknown: 3>: 6}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd60f143a60>,
                                             {<GENDER.male: 0>: 517,
                                              <GENDER.female: 1>: 260,
                                              <GENDER.unknown: 3>: 15})})
{"acc": 65.3, "f1_male": 74.4, "f1_female": 48.9, "unk_male": 6, "unk_female": 15, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 43.62%; f1_male = 58.9% (p: 46.75 / r: 79.6); f1_female = 11.78% (p: 27.83 / r: 7.47)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 85.35%, female: 13.38%, neutral: 0.0%, unknown: 1.26%
male professions = ['salesperson', 'hairdresser', 'manager', 'cleaner', 'laborer', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'sheriff']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'librarian', 'lawyer', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cashier', 'tailor', 'supervisor', 'attendant']
Unknown male: 9
Unknown female: 11
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fd4331430d0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd433143f70>,
                                           {<GENDER.male: 0>: 632,
                                            <GENDER.female: 1>: 153,
                                            <GENDER.unknown: 3>: 9}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd433143a60>,
                                             {<GENDER.male: 0>: 720,
                                              <GENDER.female: 1>: 59,
                                              <GENDER.unknown: 3>: 11})})
{"acc": 43.6, "f1_male": 58.9, "f1_female": 11.8, "unk_male": 9, "unk_female": 11, "unk_neutral": 0}
model_b
all;;;#total = 3888; 
 acc = 68.75%; f1_male = 73.99% (p: 62.83 / r: 89.98); f1_female = 67.76% (p: 84.56 / r: 56.53)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 67.26%, female: 31.33%, neutral: 0.0%, unknown: 1.41%
male professions = ['sheriff', 'technician', 'employee', 'administrator', 'onlooker', 'specialist', 'electrician', 'officer', 'plumber', 'surgeon', 'owner', 'examiner', 'dispatcher', 'bystander', 'guest', 'chef', 'doctor', 'firefighter']
female professions = ['child', 'witness', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'customer', 'someone', 'taxpayer', 'engineer', 'client', 'pedestrian', 'worker', 'student', 'educator', 'patient', 'homeowner', 'inspector', 'therapist', 'teenager', 'undergraduate', 'visitor', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'investigator', 'bartender', 'protester', 'pathologist', 'resident', 'planner', 'practitioner', 'instructor', 'veterinarian', 'paramedic', 'passenger', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'architect', 'programmer', 'paralegal', 'hygienist', 'scientist', 'dietitian', 'painter', 'broker']
Unknown male: 20
Unknown female: 33
Unknown neutral: 2
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fa090574dc0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa08fc83c10>,
                                           {<GENDER.male: 0>: 1643,
                                            <GENDER.female: 1>: 163,
                                            <GENDER.unknown: 3>: 20}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa08fc83f70>,
                                             {<GENDER.male: 0>: 759,
                                              <GENDER.female: 1>: 1030,
                                              <GENDER.unknown: 3>: 33}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa08fc83e50>,
                                              {<GENDER.male: 0>: 213,
                                               <GENDER.female: 1>: 25,
                                               <GENDER.unknown: 3>: 2})})
{"acc": 68.8, "f1_male": 74.0, "f1_female": 67.8, "unk_male": 20, "unk_female": 33, "unk_neutral": 2}
pro-stereotypical;;;#total = 1584; 
 acc = 81.94%; f1_male = 85.13% (p: 77.07 / r: 95.08); f1_female = 79.73% (p: 94.78 / r: 68.81)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 61.68%, female: 36.3%, neutral: 0.0%, unknown: 2.02%
male professions = ['developer', 'mechanic', 'chief', 'salesperson', 'lawyer', 'physician', 'farmer', 'manager', 'laborer', 'carpenter', 'janitor', 'sheriff']
female professions = ['cleaner']
neutral professions = []
ambiguous professions = ['designer', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'accountant', 'supervisor', 'attendant']
Unknown male: 9
Unknown female: 23
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f8522b7cee0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f8522b7cf70>,
                                           {<GENDER.male: 0>: 753,
                                            <GENDER.female: 1>: 30,
                                            <GENDER.unknown: 3>: 9}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f85228769d0>,
                                             {<GENDER.male: 0>: 224,
                                              <GENDER.female: 1>: 545,
                                              <GENDER.unknown: 3>: 23})})
{"acc": 81.9, "f1_male": 85.2, "f1_female": 79.7, "unk_male": 9, "unk_female": 23, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 65.85%; f1_male = 71.44% (p: 62.43 / r: 83.5); f1_female = 59.01% (p: 76.31 / r: 48.1)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 67.05%, female: 31.44%, neutral: 0.0%, unknown: 1.52%
male professions = ['librarian', 'hairdresser', 'teacher', 'editor', 'counselor', 'accountant', 'sheriff']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'carpenter', 'janitor', 'supervisor', 'attendant']
Unknown male: 13
Unknown female: 11
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fe419838c10>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe419838ee0>,
                                           {<GENDER.male: 0>: 663,
                                            <GENDER.female: 1>: 118,
                                            <GENDER.unknown: 3>: 13}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe4198381f0>,
                                             {<GENDER.male: 0>: 399,
                                              <GENDER.female: 1>: 380,
                                              <GENDER.unknown: 3>: 11})})
{"acc": 65.8, "f1_male": 71.4, "f1_female": 59.0, "unk_male": 13, "unk_female": 11, "unk_neutral": 0}
model_cbs
all;;;#total = 3888; 
 acc = 70.09%; f1_male = 75.44% (p: 65.15 / r: 89.59); f1_female = 70.19% (p: 85.01 / r: 59.77)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 64.58%, female: 32.95%, neutral: 0.0%, unknown: 2.47%
male professions = ['technician', 'employee', 'pedestrian', 'administrator', 'visitor', 'onlooker', 'specialist', 'officer', 'pathologist', 'plumber', 'surgeon', 'examiner', 'dispatcher', 'bystander', 'guest', 'doctor', 'firefighter']
female professions = ['witness']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'taxpayer', 'engineer', 'client', 'worker', 'student', 'educator', 'patient', 'homeowner', 'inspector', 'therapist', 'teenager', 'undergraduate', 'child', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'investigator', 'bartender', 'electrician', 'protester', 'victim', 'resident', 'planner', 'practitioner', 'instructor', 'owner', 'veterinarian', 'paramedic', 'passenger', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'architect', 'programmer', 'paralegal', 'hygienist', 'scientist', 'dietitian', 'painter', 'broker', 'chef']
Unknown male: 25
Unknown female: 59
Unknown neutral: 12
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7faa31cc7700>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7faa32342dc0>,
                                           {<GENDER.male: 0>: 1636,
                                            <GENDER.female: 1>: 165,
                                            <GENDER.unknown: 3>: 25}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7faa323429d0>,
                                             {<GENDER.male: 0>: 674,
                                              <GENDER.female: 1>: 1089,
                                              <GENDER.unknown: 3>: 59}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7faa32342f70>,
                                              {<GENDER.male: 0>: 201,
                                               <GENDER.female: 1>: 27,
                                               <GENDER.unknown: 3>: 12})})
{"acc": 70.1, "f1_male": 75.5, "f1_female": 70.2, "unk_male": 25, "unk_female": 59, "unk_neutral": 12}
pro-stereotypical;;;#total = 1584; 
 acc = 80.68%; f1_male = 84.44% (p: 77.2 / r: 93.18); f1_female = 78.37% (p: 92.15 / r: 68.18)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 60.35%, female: 36.99%, neutral: 0.0%, unknown: 2.65%
male professions = ['developer', 'mechanic', 'chief', 'salesperson', 'lawyer', 'physician', 'farmer', 'manager', 'laborer', 'carpenter', 'janitor', 'sheriff']
female professions = ['secretary', 'cleaner']
neutral professions = []
ambiguous professions = ['designer', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'accountant', 'supervisor', 'attendant']
Unknown male: 8
Unknown female: 34
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f9457375f70>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9457377310>,
                                           {<GENDER.male: 0>: 738,
                                            <GENDER.female: 1>: 46,
                                            <GENDER.unknown: 3>: 8}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9457377c10>,
                                             {<GENDER.male: 0>: 218,
                                              <GENDER.female: 1>: 540,
                                              <GENDER.unknown: 3>: 34})})
{"acc": 80.7, "f1_male": 84.4, "f1_female": 78.4, "unk_male": 8, "unk_female": 34, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 71.59%; f1_male = 76.41% (p: 68.57 / r: 86.27); f1_female = 67.37% (p: 82.69 / r: 56.84)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 63.07%, female: 34.28%, neutral: 0.0%, unknown: 2.65%
male professions = ['hairdresser', 'teacher', 'editor']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 15
Unknown female: 27
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f8a7b84b4c0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f8a7b84b280>,
                                           {<GENDER.male: 0>: 685,
                                            <GENDER.female: 1>: 94,
                                            <GENDER.unknown: 3>: 15}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f8a7b84b940>,
                                             {<GENDER.male: 0>: 314,
                                              <GENDER.female: 1>: 449,
                                              <GENDER.unknown: 3>: 27})})
{"acc": 71.6, "f1_male": 76.4, "f1_female": 67.3, "unk_male": 15, "unk_female": 27, "unk_neutral": 0}
padrao-mbart-finetuned-news_commentary
all;;;#total = 3888; 
 acc = 54.35%; f1_male = 66.64% (p: 52.71 / r: 90.58); f1_female = 38.28% (p: 79.69 / r: 25.19)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 80.71%, female: 14.81%, neutral: 0.0%, unknown: 4.48%
male professions = ['physician', 'customer', 'technician', 'taxpayer', 'employee', 'engineer', 'client', 'pedestrian', 'worker', 'student', 'educator', 'patient', 'inspector', 'teenager', 'undergraduate', 'administrator', 'visitor', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'onlooker', 'investigator', 'specialist', 'electrician', 'officer', 'protester', 'pathologist', 'resident', 'planner', 'practitioner', 'plumber', 'instructor', 'surgeon', 'owner', 'veterinarian', 'paramedic', 'passenger', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'architect', 'programmer', 'hygienist', 'scientist', 'dispatcher', 'bystander', 'dietitian', 'painter', 'broker', 'guest', 'chef', 'doctor', 'firefighter']
female professions = ['nurse', 'witness', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'someone', 'homeowner', 'therapist', 'child', 'bartender', 'paralegal']
Unknown male: 65
Unknown female: 105
Unknown neutral: 4
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fd4c7455160>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd4c7455940>,
                                           {<GENDER.male: 0>: 1654,
                                            <GENDER.female: 1>: 107,
                                            <GENDER.unknown: 3>: 65}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd4c7455310>,
                                             {<GENDER.male: 0>: 1258,
                                              <GENDER.female: 1>: 459,
                                              <GENDER.unknown: 3>: 105}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd4c7455670>,
                                              {<GENDER.male: 0>: 226,
                                               <GENDER.female: 1>: 10,
                                               <GENDER.unknown: 3>: 4})})
{"acc": 54.3, "f1_male": 66.6, "f1_female": 38.3, "unk_male": 65, "unk_female": 105, "unk_neutral": 4}
pro-stereotypical;;;#total = 1584; 
 acc = 68.69%; f1_male = 77.99% (p: 65.69 / r: 95.96); f1_female = 57.84% (p: 95.91 / r: 41.41)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 73.04%, female: 21.59%, neutral: 0.0%, unknown: 5.37%
male professions = ['analyst', 'salesperson', 'lawyer', 'physician', 'laborer', 'supervisor', 'sheriff']
female professions = ['nurse']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'assistant', 'chief', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'attendant']
Unknown male: 18
Unknown female: 67
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f7a3b12b040>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f7a3b12b160>,
                                           {<GENDER.male: 0>: 760,
                                            <GENDER.female: 1>: 14,
                                            <GENDER.unknown: 3>: 18}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f7a3b12bb80>,
                                             {<GENDER.male: 0>: 397,
                                              <GENDER.female: 1>: 328,
                                              <GENDER.unknown: 3>: 67})})
{"acc": 68.7, "f1_male": 78.0, "f1_female": 57.8, "unk_male": 18, "unk_female": 67, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 49.81%; f1_male = 63.39% (p: 50.76 / r: 84.38); f1_female = 23.82% (p: 56.94 / r: 15.06)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 83.33%, female: 13.19%, neutral: 0.0%, unknown: 3.47%
male professions = ['hairdresser', 'teacher', 'physician', 'secretary']
female professions = ['nurse']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'cook', 'baker', 'farmer', 'ceo', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 34
Unknown female: 21
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fdfd0883790>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fdfd0883310>,
                                           {<GENDER.male: 0>: 670,
                                            <GENDER.female: 1>: 90,
                                            <GENDER.unknown: 3>: 34}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fdfd08839d0>,
                                             {<GENDER.male: 0>: 650,
                                              <GENDER.female: 1>: 119,
                                              <GENDER.unknown: 3>: 21})})
{"acc": 49.8, "f1_male": 63.4, "f1_female": 23.9, "unk_male": 34, "unk_female": 21, "unk_neutral": 0}
padrao-mbart-finetuned-opus_books
all;;;#total = 3888; 
 acc = 52.34%; f1_male = 64.5% (p: 51.24 / r: 87.02); f1_female = 36.29% (p: 70.13 / r: 24.48)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 79.76%, female: 16.36%, neutral: 0.0%, unknown: 3.88%
male professions = ['developer', 'physician', 'customer', 'technician', 'taxpayer', 'employee', 'engineer', 'client', 'pedestrian', 'worker', 'student', 'educator', 'patient', 'homeowner', 'inspector', 'administrator', 'visitor', 'advisor', 'pharmacist', 'onlooker', 'investigator', 'bartender', 'specialist', 'electrician', 'officer', 'protester', 'pathologist', 'resident', 'planner', 'practitioner', 'plumber', 'instructor', 'surgeon', 'owner', 'veterinarian', 'passenger', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'architect', 'programmer', 'paralegal', 'hygienist', 'dispatcher', 'bystander', 'dietitian', 'painter', 'broker', 'guest', 'chef', 'doctor', 'firefighter']
female professions = ['child', 'witness', 'victim']
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'someone', 'therapist', 'teenager', 'undergraduate', 'advisee', 'psychologist', 'paramedic', 'nutritionist', 'scientist']
Unknown male: 62
Unknown female: 82
Unknown neutral: 7
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fd95485dd30>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd948462790>,
                                           {<GENDER.male: 0>: 1589,
                                            <GENDER.female: 1>: 175,
                                            <GENDER.unknown: 3>: 62}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd948462f70>,
                                             {<GENDER.male: 0>: 1294,
                                              <GENDER.female: 1>: 446,
                                              <GENDER.unknown: 3>: 82}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd9484628b0>,
                                              {<GENDER.male: 0>: 218,
                                               <GENDER.female: 1>: 15,
                                               <GENDER.unknown: 3>: 7})})
{"acc": 52.3, "f1_male": 64.5, "f1_female": 36.3, "unk_male": 62, "unk_female": 82, "unk_neutral": 7}
pro-stereotypical;;;#total = 1584; 
 acc = 66.04%; f1_male = 75.28% (p: 63.03 / r: 93.43); f1_female = 53.88% (p: 88.95 / r: 38.64)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 74.12%, female: 21.72%, neutral: 0.0%, unknown: 4.17%
male professions = ['developer', 'analyst', 'chief', 'lawyer', 'physician', 'manager', 'supervisor', 'sheriff']
female professions = ['secretary']
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'assistant', 'salesperson', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'attendant']
Unknown male: 14
Unknown female: 52
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fa2e1981280>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa2c8106160>,
                                           {<GENDER.male: 0>: 740,
                                            <GENDER.female: 1>: 38,
                                            <GENDER.unknown: 3>: 14}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa2c8106dc0>,
                                             {<GENDER.male: 0>: 434,
                                              <GENDER.female: 1>: 306,
                                              <GENDER.unknown: 3>: 52})})
{"acc": 66.0, "f1_male": 75.2, "f1_female": 53.8, "unk_male": 14, "unk_female": 52, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 47.22%; f1_male = 60.56% (p: 49.06 / r: 79.09); f1_female = 23.03% (p: 47.62 / r: 15.19)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 80.81%, female: 15.91%, neutral: 0.0%, unknown: 3.28%
male professions = ['developer', 'designer', 'physician', 'writer', 'counselor', 'janitor', 'sheriff']
female professions = []
neutral professions = []
ambiguous professions = ['mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'construction worker', 'carpenter', 'accountant', 'supervisor', 'attendant']
Unknown male: 34
Unknown female: 18
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fc76ab744c0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fc76ab74c10>,
                                           {<GENDER.male: 0>: 628,
                                            <GENDER.female: 1>: 132,
                                            <GENDER.unknown: 3>: 34}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fc76ab74dc0>,
                                             {<GENDER.male: 0>: 652,
                                              <GENDER.female: 1>: 120,
                                              <GENDER.unknown: 3>: 18})})
{"acc": 47.2, "f1_male": 60.6, "f1_female": 23.0, "unk_male": 34, "unk_female": 18, "unk_neutral": 0}
padrao-unicamp-finetuned-news_commentary
all;;;#total = 3888; 
 acc = 61.63%; f1_male = 70.59% (p: 57.76 / r: 90.74); f1_female = 54.68% (p: 83.88 / r: 40.56)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 73.79%, female: 22.66%, neutral: 0.0%, unknown: 3.55%
male professions = ['mover', 'taxpayer', 'employee', 'client', 'pedestrian', 'homeowner', 'inspector', 'visitor', 'advisee', 'advisor', 'investigator', 'electrician', 'officer', 'protester', 'pathologist', 'resident', 'planner', 'plumber', 'instructor', 'surgeon', 'owner', 'veterinarian', 'paramedic', 'passenger', 'examiner', 'buyer', 'architect', 'paralegal', 'bystander', 'painter', 'guest']
female professions = ['therapist', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'engineer', 'worker', 'student', 'educator', 'patient', 'teenager', 'undergraduate', 'administrator', 'child', 'pharmacist', 'psychologist', 'onlooker', 'witness', 'bartender', 'specialist', 'practitioner', 'chemist', 'machinist', 'appraiser', 'nutritionist', 'programmer', 'hygienist', 'scientist', 'dispatcher', 'dietitian', 'broker', 'chef', 'doctor', 'firefighter']
Unknown male: 46
Unknown female: 63
Unknown neutral: 29
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fd40465c790>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd403c36b80>,
                                           {<GENDER.male: 0>: 1657,
                                            <GENDER.female: 1>: 123,
                                            <GENDER.unknown: 3>: 46}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd40465c3a0>,
                                             {<GENDER.male: 0>: 1020,
                                              <GENDER.female: 1>: 739,
                                              <GENDER.unknown: 3>: 63}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd40465c8b0>,
                                              {<GENDER.male: 0>: 192,
                                               <GENDER.female: 1>: 19,
                                               <GENDER.unknown: 3>: 29})})
{"acc": 61.6, "f1_male": 70.6, "f1_female": 54.7, "unk_male": 46, "unk_female": 63, "unk_neutral": 29}
pro-stereotypical;;;#total = 1584; 
 acc = 63.07%; f1_male = 71.24% (p: 59.61 / r: 88.51); f1_female = 50.77% (p: 78.01 / r: 37.63)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 74.24%, female: 24.12%, neutral: 0.0%, unknown: 1.64%
male professions = ['developer', 'mover', 'analyst', 'salesperson', 'lawyer', 'physician', 'driver', 'laborer', 'construction worker', 'carpenter', 'supervisor']
female professions = []
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'housekeeper', 'assistant', 'chief', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'counselor', 'janitor', 'accountant', 'attendant', 'sheriff']
Unknown male: 7
Unknown female: 19
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fa2b3b77670>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa2b3b77550>,
                                           {<GENDER.male: 0>: 701,
                                            <GENDER.female: 1>: 84,
                                            <GENDER.unknown: 3>: 7}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa2b3bd1b80>,
                                             {<GENDER.male: 0>: 475,
                                              <GENDER.female: 1>: 298,
                                              <GENDER.unknown: 3>: 19})})
{"acc": 63.1, "f1_male": 71.2, "f1_female": 50.7, "unk_male": 7, "unk_female": 19, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 70.14%; f1_male = 76.58% (p: 64.27 / r: 94.71); f1_female = 61.0% (p: 92.76 / r: 45.44)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 73.86%, female: 24.43%, neutral: 0.0%, unknown: 1.7%
male professions = ['mover', 'librarian', 'hairdresser', 'teacher', 'editor']
female professions = ['mechanic']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'clerk', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 14
Unknown female: 13
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7ff0addacf70>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7ff0addae280>,
                                           {<GENDER.male: 0>: 752,
                                            <GENDER.female: 1>: 28,
                                            <GENDER.unknown: 3>: 14}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7ff0addae8b0>,
                                             {<GENDER.male: 0>: 418,
                                              <GENDER.female: 1>: 359,
                                              <GENDER.unknown: 3>: 13})})
{"acc": 70.1, "f1_male": 76.6, "f1_female": 61.0, "unk_male": 14, "unk_female": 13, "unk_neutral": 0}
padrao-unicamp-finetuned-opus_books
all;;;#total = 3888; 
 acc = 63.76%; f1_male = 73.05% (p: 62.58 / r: 87.73); f1_female = 61.45% (p: 84.98 / r: 48.13)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 65.84%, female: 26.54%, neutral: 0.0%, unknown: 7.61%
male professions = ['taxpayer', 'employee', 'pedestrian', 'homeowner', 'inspector', 'advisee', 'electrician', 'protester', 'pathologist', 'resident', 'planner', 'surgeon', 'veterinarian', 'paramedic', 'passenger', 'paralegal', 'painter', 'guest', 'chef', 'firefighter']
female professions = ['victim', 'dietitian']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'engineer', 'client', 'worker', 'student', 'educator', 'patient', 'therapist', 'teenager', 'undergraduate', 'administrator', 'visitor', 'child', 'advisor', 'pharmacist', 'psychologist', 'onlooker', 'witness', 'investigator', 'bartender', 'specialist', 'officer', 'practitioner', 'plumber', 'instructor', 'owner', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'architect', 'programmer', 'hygienist', 'scientist', 'dispatcher', 'bystander', 'broker', 'doctor']
Unknown male: 93
Unknown female: 157
Unknown neutral: 46
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f885d483ca0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f885d483c10>,
                                           {<GENDER.male: 0>: 1602,
                                            <GENDER.female: 1>: 131,
                                            <GENDER.unknown: 3>: 93}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f885d483d30>,
                                             {<GENDER.male: 0>: 788,
                                              <GENDER.female: 1>: 877,
                                              <GENDER.unknown: 3>: 157}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f885d483a60>,
                                              {<GENDER.male: 0>: 170,
                                               <GENDER.female: 1>: 24,
                                               <GENDER.unknown: 3>: 46})})
{"acc": 63.8, "f1_male": 73.1, "f1_female": 61.4, "unk_male": 93, "unk_female": 157, "unk_neutral": 46}
pro-stereotypical;;;#total = 1584; 
 acc = 67.49%; f1_male = 75.2% (p: 65.72 / r: 87.88); f1_female = 59.92% (p: 82.34 / r: 47.1)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 66.86%, female: 28.6%, neutral: 0.0%, unknown: 4.55%
male professions = ['mover', 'lawyer', 'manager', 'driver', 'laborer']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 16
Unknown female: 56
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f868cbfa820>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f868cbfa9d0>,
                                           {<GENDER.male: 0>: 696,
                                            <GENDER.female: 1>: 80,
                                            <GENDER.unknown: 3>: 16}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f868cbfae50>,
                                             {<GENDER.male: 0>: 363,
                                              <GENDER.female: 1>: 373,
                                              <GENDER.unknown: 3>: 56})})
{"acc": 67.5, "f1_male": 75.2, "f1_female": 59.9, "unk_male": 16, "unk_female": 56, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 72.79%; f1_male = 78.8% (p: 69.45 / r: 91.06); f1_female = 67.88% (p: 90.15 / r: 54.43)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 65.72%, female: 30.11%, neutral: 0.0%, unknown: 4.17%
male professions = ['hairdresser', 'teacher']
female professions = ['mechanic']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 24
Unknown female: 42
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fecf8cbadc0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fecf88e3700>,
                                           {<GENDER.male: 0>: 723,
                                            <GENDER.female: 1>: 47,
                                            <GENDER.unknown: 3>: 24}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fed052450d0>,
                                             {<GENDER.male: 0>: 318,
                                              <GENDER.female: 1>: 430,
                                              <GENDER.unknown: 3>: 42})})
{"acc": 72.8, "f1_male": 78.8, "f1_female": 67.8, "unk_male": 24, "unk_female": 42, "unk_neutral": 0}
padrao-unicamp-vanessa-finetuned-handscrafted
all;;;#total = 3888; 
 acc = 71.48%; f1_male = 77.0% (p: 67.17 / r: 90.2); f1_female = 72.91% (p: 88.23 / r: 62.13)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 63.07%, female: 33.0%, neutral: 0.0%, unknown: 3.94%
male professions = ['employee', 'client', 'pedestrian', 'homeowner', 'inspector', 'protester', 'pathologist', 'resident', 'planner', 'surgeon', 'veterinarian', 'passenger', 'paralegal', 'guest', 'firefighter']
female professions = ['therapist', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'taxpayer', 'engineer', 'worker', 'student', 'educator', 'patient', 'teenager', 'undergraduate', 'administrator', 'visitor', 'child', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'onlooker', 'witness', 'investigator', 'bartender', 'specialist', 'electrician', 'officer', 'practitioner', 'plumber', 'instructor', 'owner', 'paramedic', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'architect', 'programmer', 'hygienist', 'scientist', 'dispatcher', 'bystander', 'dietitian', 'painter', 'broker', 'chef', 'doctor']
Unknown male: 50
Unknown female: 77
Unknown neutral: 26
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fbcd95abe50>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fbcd9e193a0>,
                                           {<GENDER.male: 0>: 1647,
                                            <GENDER.female: 1>: 129,
                                            <GENDER.unknown: 3>: 50}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fbcd9e199d0>,
                                             {<GENDER.male: 0>: 613,
                                              <GENDER.female: 1>: 1132,
                                              <GENDER.unknown: 3>: 77}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fbcd95aba60>,
                                              {<GENDER.male: 0>: 192,
                                               <GENDER.female: 1>: 22,
                                               <GENDER.unknown: 3>: 26})})
{"acc": 71.5, "f1_male": 77.0, "f1_female": 72.9, "unk_male": 50, "unk_female": 77, "unk_neutral": 26}
pro-stereotypical;;;#total = 1584; 
 acc = 72.73%; f1_male = 77.71% (p: 69.41 / r: 88.26); f1_female = 68.17% (p: 84.36 / r: 57.2)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 63.57%, female: 33.9%, neutral: 0.0%, unknown: 2.53%
male professions = ['developer', 'mover', 'analyst', 'salesperson', 'lawyer', 'physician', 'driver', 'laborer', 'construction worker', 'supervisor']
female professions = []
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'housekeeper', 'assistant', 'chief', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'counselor', 'carpenter', 'janitor', 'accountant', 'attendant', 'sheriff']
Unknown male: 9
Unknown female: 31
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f85fe2b43a0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f85fe2b4a60>,
                                           {<GENDER.male: 0>: 699,
                                            <GENDER.female: 1>: 84,
                                            <GENDER.unknown: 3>: 9}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f85fe2b4e50>,
                                             {<GENDER.male: 0>: 308,
                                              <GENDER.female: 1>: 453,
                                              <GENDER.unknown: 3>: 31})})
{"acc": 72.7, "f1_male": 77.7, "f1_female": 68.2, "unk_male": 9, "unk_female": 31, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 81.94%; f1_male = 85.25% (p: 77.84 / r: 94.21); f1_female = 80.17% (p: 94.5 / r: 69.62)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 60.67%, female: 36.74%, neutral: 0.0%, unknown: 2.59%
male professions = ['librarian', 'hairdresser', 'teacher', 'editor', 'secretary']
female professions = ['mechanic']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 14
Unknown female: 27
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fe77294e700>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe77294e550>,
                                           {<GENDER.male: 0>: 748,
                                            <GENDER.female: 1>: 32,
                                            <GENDER.unknown: 3>: 14}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe77294e4c0>,
                                             {<GENDER.male: 0>: 213,
                                              <GENDER.female: 1>: 550,
                                              <GENDER.unknown: 3>: 27})})
{"acc": 81.9, "f1_male": 85.2, "f1_female": 80.2, "unk_male": 14, "unk_female": 27, "unk_neutral": 0}
pt-unicamp-handcrafted
all;;;#total = 3888; 
 acc = 72.69%; f1_male = 77.65% (p: 68.07 / r: 90.36); f1_female = 74.43% (p: 87.89 / r: 64.54)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 62.35%, female: 34.41%, neutral: 0.0%, unknown: 3.24%
male professions = ['employee', 'engineer', 'client', 'pedestrian', 'homeowner', 'inspector', 'specialist', 'protester', 'pathologist', 'resident', 'planner', 'plumber', 'surgeon', 'veterinarian', 'passenger', 'architect', 'paralegal', 'guest', 'firefighter']
female professions = ['victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'taxpayer', 'worker', 'student', 'educator', 'patient', 'therapist', 'teenager', 'undergraduate', 'administrator', 'visitor', 'child', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'onlooker', 'witness', 'investigator', 'bartender', 'electrician', 'officer', 'practitioner', 'instructor', 'owner', 'paramedic', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'programmer', 'hygienist', 'scientist', 'dispatcher', 'bystander', 'dietitian', 'painter', 'broker', 'chef', 'doctor']
Unknown male: 37
Unknown female: 77
Unknown neutral: 12
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fd73b55a940>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd73ada9700>,
                                           {<GENDER.male: 0>: 1650,
                                            <GENDER.female: 1>: 139,
                                            <GENDER.unknown: 3>: 37}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd73b55a8b0>,
                                             {<GENDER.male: 0>: 569,
                                              <GENDER.female: 1>: 1176,
                                              <GENDER.unknown: 3>: 77}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd73b55ad30>,
                                              {<GENDER.male: 0>: 205,
                                               <GENDER.female: 1>: 23,
                                               <GENDER.unknown: 3>: 12})})
{"acc": 72.7, "f1_male": 77.7, "f1_female": 74.4, "unk_male": 37, "unk_female": 77, "unk_neutral": 12}
pro-stereotypical;;;#total = 1584; 
 acc = 74.56%; f1_male = 78.8% (p: 71.34 / r: 88.01); f1_female = 71.07% (p: 84.91 / r: 61.11)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 61.68%, female: 35.98%, neutral: 0.0%, unknown: 2.34%
male professions = ['developer', 'mover', 'analyst', 'salesperson', 'lawyer', 'physician', 'driver', 'laborer', 'construction worker', 'carpenter', 'supervisor', 'sheriff']
female professions = []
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'housekeeper', 'assistant', 'chief', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'counselor', 'janitor', 'accountant', 'attendant']
Unknown male: 9
Unknown female: 28
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fbf8203fee0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fbf8203fa60>,
                                           {<GENDER.male: 0>: 697,
                                            <GENDER.female: 1>: 86,
                                            <GENDER.unknown: 3>: 9}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fbf8203f820>,
                                             {<GENDER.male: 0>: 280,
                                              <GENDER.female: 1>: 484,
                                              <GENDER.unknown: 3>: 28})})
{"acc": 74.6, "f1_male": 78.8, "f1_female": 71.1, "unk_male": 9, "unk_female": 28, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 82.32%; f1_male = 85.6% (p: 79.14 / r: 93.2); f1_female = 80.8% (p: 93.07 / r: 71.39)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 59.03%, female: 38.26%, neutral: 0.0%, unknown: 2.71%
male professions = ['hairdresser', 'teacher', 'editor', 'secretary']
female professions = ['mechanic']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 12
Unknown female: 31
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fd586374ee0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd586376280>,
                                           {<GENDER.male: 0>: 740,
                                            <GENDER.female: 1>: 42,
                                            <GENDER.unknown: 3>: 12}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd586376040>,
                                             {<GENDER.male: 0>: 195,
                                              <GENDER.female: 1>: 564,
                                              <GENDER.unknown: 3>: 31})})
{"acc": 82.3, "f1_male": 85.6, "f1_female": 80.8, "unk_male": 12, "unk_female": 31, "unk_neutral": 0}
pt-unicamp-handcrafted-puro
all;;;#total = 3888; 
 acc = 72.99%; f1_male = 79.48% (p: 71.55 / r: 89.38); f1_female = 75.54% (p: 87.96 / r: 66.19)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 58.67%, female: 35.26%, neutral: 0.0%, unknown: 6.07%
male professions = ['employee', 'client', 'pedestrian', 'inspector', 'bartender', 'specialist', 'electrician', 'protester', 'pathologist', 'resident', 'plumber', 'surgeon', 'veterinarian', 'passenger', 'paralegal', 'painter', 'guest']
female professions = ['therapist', 'undergraduate', 'witness', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'taxpayer', 'engineer', 'worker', 'student', 'educator', 'patient', 'homeowner', 'teenager', 'administrator', 'visitor', 'child', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'onlooker', 'investigator', 'officer', 'planner', 'practitioner', 'instructor', 'owner', 'paramedic', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'architect', 'programmer', 'hygienist', 'scientist', 'dispatcher', 'bystander', 'dietitian', 'broker', 'chef', 'doctor', 'firefighter']
Unknown male: 67
Unknown female: 135
Unknown neutral: 34
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f81a1d8d310>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f81a1d78700>,
                                           {<GENDER.male: 0>: 1632,
                                            <GENDER.female: 1>: 127,
                                            <GENDER.unknown: 3>: 67}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f81a1d78670>,
                                             {<GENDER.male: 0>: 481,
                                              <GENDER.female: 1>: 1206,
                                              <GENDER.unknown: 3>: 135}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f81a1d8db80>,
                                              {<GENDER.male: 0>: 168,
                                               <GENDER.female: 1>: 38,
                                               <GENDER.unknown: 3>: 34})})
{"acc": 73.0, "f1_male": 79.5, "f1_female": 75.6, "unk_male": 67, "unk_female": 135, "unk_neutral": 34}
pro-stereotypical;;;#total = 1584; 
 acc = 75.44%; f1_male = 81.06% (p: 74.32 / r: 89.14); f1_female = 72.07% (p: 86.55 / r: 61.74)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 59.97%, female: 35.67%, neutral: 0.0%, unknown: 4.36%
male professions = ['developer', 'mover', 'chief', 'salesperson', 'lawyer', 'physician', 'manager', 'laborer', 'supervisor']
female professions = []
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'housekeeper', 'analyst', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'attendant', 'sheriff']
Unknown male: 10
Unknown female: 59
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7ff77cb6ff70>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7ff77cb713a0>,
                                           {<GENDER.male: 0>: 706,
                                            <GENDER.female: 1>: 76,
                                            <GENDER.unknown: 3>: 10}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7ff77c87fa60>,
                                             {<GENDER.male: 0>: 244,
                                              <GENDER.female: 1>: 489,
                                              <GENDER.unknown: 3>: 59})})
{"acc": 75.4, "f1_male": 81.0, "f1_female": 72.0, "unk_male": 10, "unk_female": 59, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 83.21%; f1_male = 87.0% (p: 81.0 / r: 93.95); f1_female = 82.19% (p: 95.02 / r: 72.41)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 58.14%, female: 38.01%, neutral: 0.0%, unknown: 3.85%
male professions = ['receptionist', 'writer']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 18
Unknown female: 43
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7ff1d7b70ee0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7ff1d7b72820>,
                                           {<GENDER.male: 0>: 746,
                                            <GENDER.female: 1>: 30,
                                            <GENDER.unknown: 3>: 18}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7ff1d7b72310>,
                                             {<GENDER.male: 0>: 175,
                                              <GENDER.female: 1>: 572,
                                              <GENDER.unknown: 3>: 43})})
{"acc": 83.2, "f1_male": 87.0, "f1_female": 82.2, "unk_male": 18, "unk_female": 43, "unk_neutral": 0}
systran
all;;;#total = 3888; 
 acc = 60.19%; f1_male = 68.99% (p: 55.53 / r: 91.07); f1_female = 50.91% (p: 80.79 / r: 37.16)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 77.03%, female: 21.55%, neutral: 0.0%, unknown: 1.41%
male professions = ['mover', 'sheriff', 'technician', 'taxpayer', 'engineer', 'client', 'pedestrian', 'inspector', 'advisee', 'onlooker', 'specialist', 'electrician', 'officer', 'pathologist', 'planner', 'plumber', 'instructor', 'surgeon', 'veterinarian', 'paramedic', 'passenger', 'examiner', 'buyer', 'architect', 'dispatcher', 'bystander', 'dietitian', 'chef', 'doctor', 'firefighter']
female professions = ['therapist', 'undergraduate', 'child', 'witness', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'customer', 'someone', 'employee', 'worker', 'student', 'educator', 'patient', 'homeowner', 'teenager', 'administrator', 'visitor', 'advisor', 'pharmacist', 'psychologist', 'investigator', 'bartender', 'protester', 'resident', 'practitioner', 'owner', 'chemist', 'machinist', 'appraiser', 'nutritionist', 'programmer', 'paralegal', 'hygienist', 'scientist', 'painter', 'broker', 'guest']
Unknown male: 22
Unknown female: 30
Unknown neutral: 3
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f946b375e50>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f946ad9f820>,
                                           {<GENDER.male: 0>: 1663,
                                            <GENDER.female: 1>: 141,
                                            <GENDER.unknown: 3>: 22}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f946b375b80>,
                                             {<GENDER.male: 0>: 1115,
                                              <GENDER.female: 1>: 677,
                                              <GENDER.unknown: 3>: 30}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f946adbe700>,
                                              {<GENDER.male: 0>: 217,
                                               <GENDER.female: 1>: 20,
                                               <GENDER.unknown: 3>: 3})})
{"acc": 60.2, "f1_male": 69.0, "f1_female": 50.9, "unk_male": 22, "unk_female": 30, "unk_neutral": 3}
pro-stereotypical;;;#total = 1584; 
 acc = 73.61%; f1_male = 79.39% (p: 66.84 / r: 97.73); f1_female = 65.55% (p: 97.03 / r: 49.49)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 73.11%, female: 25.51%, neutral: 0.0%, unknown: 1.39%
male professions = ['developer', 'mover', 'analyst', 'chief', 'salesperson', 'lawyer', 'physician', 'manager', 'laborer', 'construction worker', 'carpenter', 'supervisor', 'sheriff']
female professions = ['secretary']
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'housekeeper', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'cleaner', 'cashier', 'tailor', 'writer', 'counselor', 'janitor', 'accountant', 'attendant']
Unknown male: 6
Unknown female: 16
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fbe644b6700>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fbe644b69d0>,
                                           {<GENDER.male: 0>: 774,
                                            <GENDER.female: 1>: 12,
                                            <GENDER.unknown: 3>: 6}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fbe644b6430>,
                                             {<GENDER.male: 0>: 384,
                                              <GENDER.female: 1>: 392,
                                              <GENDER.unknown: 3>: 16})})
{"acc": 73.6, "f1_male": 79.3, "f1_female": 65.5, "unk_male": 6, "unk_female": 16, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 54.36%; f1_male = 65.32% (p: 53.49 / r: 83.88); f1_female = 35.42% (p: 62.7 / r: 24.68)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 78.6%, female: 19.63%, neutral: 0.0%, unknown: 1.77%
male professions = ['mover', 'assistant', 'salesperson', 'hairdresser', 'teacher', 'carpenter', 'sheriff']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'housekeeper', 'analyst', 'chief', 'librarian', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'janitor', 'accountant', 'supervisor', 'attendant']
Unknown male: 12
Unknown female: 16
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fe1bd821f70>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe1bd821430>,
                                           {<GENDER.male: 0>: 666,
                                            <GENDER.female: 1>: 116,
                                            <GENDER.unknown: 3>: 12}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe1bd8211f0>,
                                             {<GENDER.male: 0>: 579,
                                              <GENDER.female: 1>: 195,
                                              <GENDER.unknown: 3>: 16})})
{"acc": 54.4, "f1_male": 65.3, "f1_female": 35.4, "unk_male": 12, "unk_female": 16, "unk_neutral": 0}
unicamp-dl-translation-en-pt-t5
all;;;#total = 3888; 
 acc = 68.42%; f1_male = 76.25% (p: 66.37 / r: 89.59); f1_female = 68.4% (p: 87.37 / r: 56.2)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 63.4%, female: 30.14%, neutral: 0.0%, unknown: 6.46%
male professions = ['taxpayer', 'employee', 'client', 'pedestrian', 'homeowner', 'inspector', 'bartender', 'specialist', 'protester', 'pathologist', 'resident', 'planner', 'plumber', 'surgeon', 'owner', 'veterinarian', 'passenger', 'paralegal', 'painter', 'guest']
female professions = ['therapist', 'witness', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'engineer', 'worker', 'student', 'educator', 'patient', 'teenager', 'undergraduate', 'administrator', 'visitor', 'child', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'onlooker', 'investigator', 'electrician', 'officer', 'practitioner', 'instructor', 'paramedic', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'architect', 'programmer', 'hygienist', 'scientist', 'dispatcher', 'bystander', 'dietitian', 'broker', 'chef', 'doctor', 'firefighter']
Unknown male: 77
Unknown female: 141
Unknown neutral: 33
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fe513c55b80>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe5135688b0>,
                                           {<GENDER.male: 0>: 1636,
                                            <GENDER.female: 1>: 113,
                                            <GENDER.unknown: 3>: 77}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe513c55ee0>,
                                             {<GENDER.male: 0>: 657,
                                              <GENDER.female: 1>: 1024,
                                              <GENDER.unknown: 3>: 141}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe513590af0>,
                                              {<GENDER.male: 0>: 172,
                                               <GENDER.female: 1>: 35,
                                               <GENDER.unknown: 3>: 33})})
{"acc": 68.4, "f1_male": 76.3, "f1_female": 68.4, "unk_male": 77, "unk_female": 141, "unk_neutral": 33}
pro-stereotypical;;;#total = 1584; 
 acc = 70.2%; f1_male = 77.62% (p: 68.88 / r: 88.89); f1_female = 64.05% (p: 84.65 / r: 51.52)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 64.52%, female: 30.43%, neutral: 0.0%, unknown: 5.05%
male professions = ['mover', 'chief', 'salesperson', 'lawyer', 'physician', 'laborer', 'supervisor']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'housekeeper', 'analyst', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'attendant', 'sheriff']
Unknown male: 14
Unknown female: 66
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f7b4b4a2430>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f7b4b4a2310>,
                                           {<GENDER.male: 0>: 704,
                                            <GENDER.female: 1>: 74,
                                            <GENDER.unknown: 3>: 14}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f7b4b4a23a0>,
                                             {<GENDER.male: 0>: 318,
                                              <GENDER.female: 1>: 408,
                                              <GENDER.unknown: 3>: 66})})
{"acc": 70.2, "f1_male": 77.6, "f1_female": 64.0, "unk_male": 14, "unk_female": 66, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 77.84%; f1_male = 82.93% (p: 74.23 / r: 93.95); f1_female = 74.7% (p: 94.75 / r: 61.65)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 63.45%, female: 32.45%, neutral: 0.0%, unknown: 4.1%
male professions = ['hairdresser', 'receptionist', 'secretary', 'writer']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'guard', 'editor', 'cleaner', 'laborer', 'cashier', 'tailor', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 21
Unknown female: 44
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fd93c000af0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd93bcffdc0>,
                                           {<GENDER.male: 0>: 746,
                                            <GENDER.female: 1>: 27,
                                            <GENDER.unknown: 3>: 21}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd93c000dc0>,
                                             {<GENDER.male: 0>: 259,
                                              <GENDER.female: 1>: 487,
                                              <GENDER.unknown: 3>: 44})})
{"acc": 77.8, "f1_male": 82.9, "f1_female": 74.6, "unk_male": 21, "unk_female": 44, "unk_neutral": 0}
unicamp-finetuned-en-to-pt-dataset-ted
all;;;#total = 3888; 
 acc = 44.6%; f1_male = 60.79% (p: 47.8 / r: 83.46); f1_female = 18.81% (p: 51.09 / r: 11.53)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 82.0%, female: 10.57%, neutral: 0.0%, unknown: 7.43%
male professions = ['manager', 'carpenter', 'supervisor', 'customer', 'technician', 'taxpayer', 'employee', 'engineer', 'client', 'worker', 'educator', 'homeowner', 'inspector', 'therapist', 'teenager', 'visitor', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'onlooker', 'investigator', 'specialist', 'electrician', 'officer', 'protester', 'pathologist', 'victim', 'resident', 'planner', 'plumber', 'instructor', 'surgeon', 'owner', 'paramedic', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'architect', 'programmer', 'paralegal', 'hygienist', 'scientist', 'dispatcher', 'bystander', 'painter', 'broker', 'guest', 'chef', 'doctor', 'firefighter']
female professions = ['undergraduate', 'child', 'practitioner', 'veterinarian', 'passenger', 'nutritionist']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'janitor', 'accountant', 'attendant', 'sheriff', 'someone', 'pedestrian', 'student', 'patient', 'administrator', 'witness', 'bartender', 'dietitian']
Unknown male: 123
Unknown female: 140
Unknown neutral: 26
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fac54486670>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fac53ccc940>,
                                           {<GENDER.male: 0>: 1524,
                                            <GENDER.female: 1>: 179,
                                            <GENDER.unknown: 3>: 123}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fac54486700>,
                                             {<GENDER.male: 0>: 1472,
                                              <GENDER.female: 1>: 210,
                                              <GENDER.unknown: 3>: 140}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fac54486f70>,
                                              {<GENDER.male: 0>: 192,
                                               <GENDER.female: 1>: 22,
                                               <GENDER.unknown: 3>: 26})})
{"acc": 44.6, "f1_male": 60.8, "f1_female": 18.8, "unk_male": 123, "unk_female": 140, "unk_neutral": 26}
pro-stereotypical;;;#total = 1584; 
 acc = 49.56%; f1_male = 64.43% (p: 51.43 / r: 86.24); f1_female = 21.14% (p: 58.96 / r: 12.88)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 83.84%, female: 10.92%, neutral: 0.0%, unknown: 5.24%
male professions = ['manager', 'editor', 'carpenter', 'supervisor']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'janitor', 'accountant', 'attendant', 'sheriff']
Unknown male: 38
Unknown female: 45
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f9a7ecbd550>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9a7ecbd0d0>,
                                           {<GENDER.male: 0>: 683,
                                            <GENDER.female: 1>: 71,
                                            <GENDER.unknown: 3>: 38}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9a7ecbd4c0>,
                                             {<GENDER.male: 0>: 645,
                                              <GENDER.female: 1>: 102,
                                              <GENDER.unknown: 3>: 45})})
{"acc": 49.6, "f1_male": 64.4, "f1_female": 21.2, "unk_male": 38, "unk_female": 45, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 47.66%; f1_male = 62.8% (p: 50.23 / r: 83.75); f1_female = 18.52% (p: 49.45 / r: 11.39)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 83.59%, female: 11.49%, neutral: 0.0%, unknown: 4.92%
male professions = ['manager', 'carpenter', 'supervisor']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'janitor', 'accountant', 'attendant', 'sheriff']
Unknown male: 37
Unknown female: 41
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fdc34cb9b80>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fdc348889d0>,
                                           {<GENDER.male: 0>: 665,
                                            <GENDER.female: 1>: 92,
                                            <GENDER.unknown: 3>: 37}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fdc34cb9ee0>,
                                             {<GENDER.male: 0>: 659,
                                              <GENDER.female: 1>: 90,
                                              <GENDER.unknown: 3>: 41})})
{"acc": 47.7, "f1_male": 62.8, "f1_female": 18.5, "unk_male": 37, "unk_female": 41, "unk_neutral": 0}
unicamp_handcrafted
all;;;#total = 3888; 
 acc = 73.05%; f1_male = 77.94% (p: 68.4 / r: 90.58); f1_female = 75.06% (p: 88.64 / r: 65.09)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 62.19%, female: 34.41%, neutral: 0.0%, unknown: 3.4%
male professions = ['employee', 'engineer', 'client', 'pedestrian', 'homeowner', 'inspector', 'bartender', 'specialist', 'protester', 'pathologist', 'resident', 'planner', 'surgeon', 'veterinarian', 'passenger', 'paralegal', 'guest', 'firefighter']
female professions = ['victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'taxpayer', 'worker', 'student', 'educator', 'patient', 'therapist', 'teenager', 'undergraduate', 'administrator', 'visitor', 'child', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'onlooker', 'witness', 'investigator', 'electrician', 'officer', 'practitioner', 'plumber', 'instructor', 'owner', 'paramedic', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'architect', 'programmer', 'hygienist', 'scientist', 'dispatcher', 'bystander', 'dietitian', 'painter', 'broker', 'chef', 'doctor']
Unknown male: 40
Unknown female: 79
Unknown neutral: 13
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fac3bc59b80>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fac3b5b4af0>,
                                           {<GENDER.male: 0>: 1654,
                                            <GENDER.female: 1>: 132,
                                            <GENDER.unknown: 3>: 40}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fac3bc59dc0>,
                                             {<GENDER.male: 0>: 557,
                                              <GENDER.female: 1>: 1186,
                                              <GENDER.unknown: 3>: 79}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fac3bc59f70>,
                                              {<GENDER.male: 0>: 207,
                                               <GENDER.female: 1>: 20,
                                               <GENDER.unknown: 3>: 13})})
{"acc": 73.0, "f1_male": 78.0, "f1_female": 75.1, "unk_male": 40, "unk_female": 79, "unk_neutral": 13}
pro-stereotypical;;;#total = 1584; 
 acc = 74.43%; f1_male = 78.94% (p: 71.56 / r: 88.01); f1_female = 70.94% (p: 85.01 / r: 60.86)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 61.49%, female: 35.8%, neutral: 0.0%, unknown: 2.71%
male professions = ['developer', 'mover', 'analyst', 'salesperson', 'lawyer', 'physician', 'driver', 'laborer', 'construction worker', 'supervisor', 'sheriff']
female professions = []
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'housekeeper', 'assistant', 'chief', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'counselor', 'carpenter', 'janitor', 'accountant', 'attendant']
Unknown male: 10
Unknown female: 33
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f80848a3820>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f80848a31f0>,
                                           {<GENDER.male: 0>: 697,
                                            <GENDER.female: 1>: 85,
                                            <GENDER.unknown: 3>: 10}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f80848a38b0>,
                                             {<GENDER.male: 0>: 277,
                                              <GENDER.female: 1>: 482,
                                              <GENDER.unknown: 3>: 33})})
{"acc": 74.4, "f1_male": 79.0, "f1_female": 71.0, "unk_male": 10, "unk_female": 33, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 83.46%; f1_male = 86.26% (p: 79.91 / r: 93.7); f1_female = 82.33% (p: 94.14 / r: 73.16)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 58.78%, female: 38.76%, neutral: 0.0%, unknown: 2.46%
male professions = ['librarian', 'hairdresser', 'teacher', 'editor', 'secretary']
female professions = ['mechanic']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 14
Unknown female: 25
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fa8cf9335e0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa8cf933e50>,
                                           {<GENDER.male: 0>: 744,
                                            <GENDER.female: 1>: 36,
                                            <GENDER.unknown: 3>: 14}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa8cf933ee0>,
                                             {<GENDER.male: 0>: 187,
                                              <GENDER.female: 1>: 578,
                                              <GENDER.unknown: 3>: 25})})
{"acc": 83.5, "f1_male": 86.3, "f1_female": 82.3, "unk_male": 14, "unk_female": 25, "unk_neutral": 0}
unicamp_news
all;;;#total = 3888; 
 acc = 64.02%; f1_male = 72.0% (p: 59.37 / r: 91.46); f1_female = 58.86% (p: 85.22 / r: 44.95)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 72.35%, female: 24.72%, neutral: 0.0%, unknown: 2.93%
male professions = ['mover', 'taxpayer', 'employee', 'client', 'pedestrian', 'homeowner', 'inspector', 'advisee', 'advisor', 'bartender', 'specialist', 'electrician', 'officer', 'protester', 'pathologist', 'resident', 'planner', 'plumber', 'surgeon', 'owner', 'veterinarian', 'passenger', 'examiner', 'architect', 'paralegal', 'bystander', 'painter', 'guest']
female professions = ['victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'engineer', 'worker', 'student', 'educator', 'patient', 'therapist', 'teenager', 'undergraduate', 'administrator', 'visitor', 'child', 'pharmacist', 'psychologist', 'onlooker', 'witness', 'investigator', 'practitioner', 'instructor', 'paramedic', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'programmer', 'hygienist', 'scientist', 'dispatcher', 'dietitian', 'broker', 'chef', 'doctor', 'firefighter']
Unknown male: 34
Unknown female: 66
Unknown neutral: 14
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7ff779d5be50>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7ff779400b80>,
                                           {<GENDER.male: 0>: 1670,
                                            <GENDER.female: 1>: 122,
                                            <GENDER.unknown: 3>: 34}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7ff779400ca0>,
                                             {<GENDER.male: 0>: 937,
                                              <GENDER.female: 1>: 819,
                                              <GENDER.unknown: 3>: 66}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7ff779400f70>,
                                              {<GENDER.male: 0>: 206,
                                               <GENDER.female: 1>: 20,
                                               <GENDER.unknown: 3>: 14})})
{"acc": 64.0, "f1_male": 72.0, "f1_female": 58.9, "unk_male": 34, "unk_female": 66, "unk_neutral": 14}
pro-stereotypical;;;#total = 1584; 
 acc = 64.65%; f1_male = 72.45% (p: 61.33 / r: 88.51); f1_female = 53.88% (p: 79.36 / r: 40.78)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 72.16%, female: 25.69%, neutral: 0.0%, unknown: 2.15%
male professions = ['developer', 'mover', 'analyst', 'salesperson', 'lawyer', 'physician', 'driver', 'laborer', 'construction worker', 'supervisor']
female professions = []
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'housekeeper', 'assistant', 'chief', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'counselor', 'carpenter', 'janitor', 'accountant', 'attendant', 'sheriff']
Unknown male: 7
Unknown female: 27
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f7f13ca9f70>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f7f1f7e49d0>,
                                           {<GENDER.male: 0>: 701,
                                            <GENDER.female: 1>: 84,
                                            <GENDER.unknown: 3>: 7}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f7f1f7e4550>,
                                             {<GENDER.male: 0>: 442,
                                              <GENDER.female: 1>: 323,
                                              <GENDER.unknown: 3>: 27})})
{"acc": 64.6, "f1_male": 72.4, "f1_female": 53.9, "unk_male": 7, "unk_female": 27, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 72.47%; f1_male = 78.11% (p: 66.52 / r: 94.58); f1_female = 65.29% (p: 93.19 / r: 50.25)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 71.28%, female: 26.89%, neutral: 0.0%, unknown: 1.83%
male professions = ['mover', 'librarian', 'hairdresser', 'teacher', 'editor', 'counselor']
female professions = ['mechanic']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'clerk', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 14
Unknown female: 15
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f94f5b76ee0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9502009e50>,
                                           {<GENDER.male: 0>: 751,
                                            <GENDER.female: 1>: 29,
                                            <GENDER.unknown: 3>: 14}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f95020093a0>,
                                             {<GENDER.male: 0>: 378,
                                              <GENDER.female: 1>: 397,
                                              <GENDER.unknown: 3>: 15})})
{"acc": 72.5, "f1_male": 78.1, "f1_female": 65.3, "unk_male": 14, "unk_female": 15, "unk_neutral": 0}
aws
all;;;#total = 3888; 
 acc = 65.69%; f1_male = 71.57% (p: 59.85 / r: 88.99); f1_female = 62.75% (p: 81.56 / r: 50.99)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 69.83%, female: 29.3%, neutral: 0.0%, unknown: 0.87%
male professions = ['taxpayer', 'pedestrian', 'inspector', 'therapist', 'onlooker', 'specialist', 'electrician', 'officer', 'pathologist', 'plumber', 'surgeon', 'examiner', 'buyer', 'paralegal', 'dispatcher', 'bystander', 'doctor', 'firefighter']
female professions = ['child', 'witness', 'bartender', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'employee', 'engineer', 'client', 'worker', 'student', 'educator', 'patient', 'homeowner', 'teenager', 'undergraduate', 'administrator', 'visitor', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'investigator', 'protester', 'resident', 'planner', 'practitioner', 'instructor', 'owner', 'veterinarian', 'paramedic', 'passenger', 'chemist', 'machinist', 'appraiser', 'nutritionist', 'architect', 'programmer', 'hygienist', 'scientist', 'dietitian', 'painter', 'broker', 'guest', 'chef']
Unknown male: 13
Unknown female: 19
Unknown neutral: 2
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7febfe4a7670>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7febfe4a78b0>,
                                           {<GENDER.male: 0>: 1625,
                                            <GENDER.female: 1>: 188,
                                            <GENDER.unknown: 3>: 13}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7febfe4a79d0>,
                                             {<GENDER.male: 0>: 874,
                                              <GENDER.female: 1>: 929,
                                              <GENDER.unknown: 3>: 19}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7febfe4a7af0>,
                                              {<GENDER.male: 0>: 216,
                                               <GENDER.female: 1>: 22,
                                               <GENDER.unknown: 3>: 2})})
{"acc": 65.7, "f1_male": 71.6, "f1_female": 62.8, "unk_male": 13, "unk_female": 19, "unk_neutral": 2}
pro-stereotypical;;;#total = 1584; 
 acc = 81.12%; f1_male = 84.33% (p: 73.88 / r: 98.23); f1_female = 77.41% (p: 97.88 / r: 64.02)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 66.48%, female: 32.7%, neutral: 0.0%, unknown: 0.82%
male professions = ['developer', 'mechanic', 'analyst', 'chief', 'salesperson', 'lawyer', 'physician', 'farmer', 'manager', 'laborer', 'construction worker', 'carpenter', 'janitor', 'supervisor', 'sheriff']
female professions = ['nurse', 'cleaner']
neutral professions = []
ambiguous professions = ['designer', 'clerk', 'mover', 'housekeeper', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'ceo', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cashier', 'tailor', 'writer', 'counselor', 'accountant', 'attendant']
Unknown male: 3
Unknown female: 10
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fb095389f70>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fb0a180e040>,
                                           {<GENDER.male: 0>: 778,
                                            <GENDER.female: 1>: 11,
                                            <GENDER.unknown: 3>: 3}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fb0a180eb80>,
                                             {<GENDER.male: 0>: 275,
                                              <GENDER.female: 1>: 507,
                                              <GENDER.unknown: 3>: 10})})
{"acc": 81.1, "f1_male": 84.3, "f1_female": 77.4, "unk_male": 3, "unk_female": 10, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 58.96%; f1_male = 65.89% (p: 56.86 / r: 78.34); f1_female = 49.29% (p: 65.55 / r: 39.49)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 69.07%, female: 30.05%, neutral: 0.0%, unknown: 0.88%
male professions = ['clerk', 'assistant', 'librarian', 'hairdresser', 'teacher', 'editor', 'writer', 'accountant', 'attendant']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'mover', 'housekeeper', 'analyst', 'chief', 'salesperson', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'construction worker', 'counselor', 'carpenter', 'janitor', 'supervisor', 'sheriff']
Unknown male: 8
Unknown female: 6
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f809f38a160>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f809f38a0d0>,
                                           {<GENDER.male: 0>: 622,
                                            <GENDER.female: 1>: 164,
                                            <GENDER.unknown: 3>: 8}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f809f38a9d0>,
                                             {<GENDER.male: 0>: 472,
                                              <GENDER.female: 1>: 312,
                                              <GENDER.unknown: 3>: 6})})
{"acc": 59.0, "f1_male": 65.9, "f1_female": 49.3, "unk_male": 8, "unk_female": 6, "unk_neutral": 0}
bing
all;;;#total = 3888; 
 acc = 57.69%; f1_male = 67.51% (p: 53.83 / r: 90.53); f1_female = 45.57% (p: 76.92 / r: 32.38)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 78.99%, female: 19.73%, neutral: 0.0%, unknown: 1.29%
male professions = ['physician', 'janitor', 'sheriff', 'technician', 'taxpayer', 'employee', 'pedestrian', 'inspector', 'therapist', 'visitor', 'pharmacist', 'onlooker', 'investigator', 'specialist', 'electrician', 'officer', 'protester', 'pathologist', 'plumber', 'surgeon', 'owner', 'veterinarian', 'paramedic', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'scientist', 'dispatcher', 'bystander', 'painter', 'guest', 'doctor', 'firefighter']
female professions = ['child', 'advisee', 'witness', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'accountant', 'supervisor', 'attendant', 'customer', 'someone', 'engineer', 'client', 'worker', 'student', 'educator', 'patient', 'homeowner', 'teenager', 'undergraduate', 'administrator', 'advisor', 'psychologist', 'bartender', 'resident', 'planner', 'practitioner', 'instructor', 'passenger', 'nutritionist', 'architect', 'programmer', 'paralegal', 'hygienist', 'dietitian', 'broker', 'chef']
Unknown male: 14
Unknown female: 35
Unknown neutral: 1
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f7d36d2b040>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f7d36d2b9d0>,
                                           {<GENDER.male: 0>: 1653,
                                            <GENDER.female: 1>: 159,
                                            <GENDER.unknown: 3>: 14}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f7d36d2b670>,
                                             {<GENDER.male: 0>: 1197,
                                              <GENDER.female: 1>: 590,
                                              <GENDER.unknown: 3>: 35}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f7d36d2bdc0>,
                                              {<GENDER.male: 0>: 221,
                                               <GENDER.female: 1>: 18,
                                               <GENDER.unknown: 3>: 1})})
{"acc": 57.7, "f1_male": 67.5, "f1_female": 45.6, "unk_male": 14, "unk_female": 35, "unk_neutral": 1}
pro-stereotypical;;;#total = 1584; 
 acc = 72.1%; f1_male = 78.42% (p: 65.37 / r: 97.98); f1_female = 62.67% (p: 97.34 / r: 46.21)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 74.94%, female: 23.74%, neutral: 0.0%, unknown: 1.33%
male professions = ['developer', 'analyst', 'chief', 'salesperson', 'lawyer', 'physician', 'farmer', 'manager', 'laborer', 'carpenter', 'janitor', 'supervisor', 'sheriff']
female professions = ['secretary']
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'cleaner', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'accountant', 'attendant']
Unknown male: 6
Unknown female: 15
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f9dc537ff70>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9dc5383f70>,
                                           {<GENDER.male: 0>: 776,
                                            <GENDER.female: 1>: 10,
                                            <GENDER.unknown: 3>: 6}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9dc5383310>,
                                             {<GENDER.male: 0>: 411,
                                              <GENDER.female: 1>: 366,
                                              <GENDER.unknown: 3>: 15})})
{"acc": 72.1, "f1_male": 78.4, "f1_female": 62.7, "unk_male": 6, "unk_female": 15, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 50.13%; f1_male = 62.55% (p: 50.75 / r: 81.49); f1_female = 27.33% (p: 51.4 / r: 18.61)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 80.49%, female: 18.06%, neutral: 0.0%, unknown: 1.45%
male professions = ['assistant', 'librarian', 'hairdresser', 'teacher', 'physician', 'writer', 'counselor', 'carpenter', 'janitor', 'accountant', 'attendant', 'sheriff']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'chief', 'salesperson', 'lawyer', 'cook', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'construction worker', 'supervisor']
Unknown male: 8
Unknown female: 15
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f9d1211cc10>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9d1211c820>,
                                           {<GENDER.male: 0>: 647,
                                            <GENDER.female: 1>: 139,
                                            <GENDER.unknown: 3>: 8}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9d1211c430>,
                                             {<GENDER.male: 0>: 628,
                                              <GENDER.female: 1>: 147,
                                              <GENDER.unknown: 3>: 15})})
{"acc": 50.1, "f1_male": 62.5, "f1_female": 27.3, "unk_male": 8, "unk_female": 15, "unk_neutral": 0}
google
all;;;#total = 3888; 
 acc = 50.98%; f1_male = 63.75% (p: 49.6 / r: 89.21); f1_female = 29.72% (p: 63.83 / r: 19.37)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 84.47%, female: 14.22%, neutral: 0.0%, unknown: 1.31%
male professions = ['salesperson', 'manager', 'laborer', 'construction worker', 'janitor', 'accountant', 'sheriff', 'customer', 'technician', 'taxpayer', 'employee', 'engineer', 'client', 'pedestrian', 'homeowner', 'inspector', 'therapist', 'administrator', 'visitor', 'advisee', 'advisor', 'pharmacist', 'onlooker', 'investigator', 'specialist', 'electrician', 'officer', 'protester', 'pathologist', 'planner', 'practitioner', 'plumber', 'instructor', 'surgeon', 'owner', 'veterinarian', 'paramedic', 'passenger', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'architect', 'programmer', 'paralegal', 'scientist', 'dispatcher', 'bystander', 'dietitian', 'painter', 'broker', 'guest', 'chef', 'doctor', 'firefighter']
female professions = ['undergraduate', 'child', 'witness', 'bartender', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'counselor', 'carpenter', 'supervisor', 'attendant', 'someone', 'worker', 'student', 'educator', 'patient', 'teenager', 'psychologist', 'resident', 'nutritionist', 'hygienist']
Unknown male: 18
Unknown female: 30
Unknown neutral: 3
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f9048d31dc0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9048cbf3a0>,
                                           {<GENDER.male: 0>: 1629,
                                            <GENDER.female: 1>: 179,
                                            <GENDER.unknown: 3>: 18}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9048cbf430>,
                                             {<GENDER.male: 0>: 1439,
                                              <GENDER.female: 1>: 353,
                                              <GENDER.unknown: 3>: 30}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9048d313a0>,
                                              {<GENDER.male: 0>: 216,
                                               <GENDER.female: 1>: 21,
                                               <GENDER.unknown: 3>: 3})})
{"acc": 51.0, "f1_male": 63.8, "f1_female": 29.8, "unk_male": 18, "unk_female": 30, "unk_neutral": 3}
pro-stereotypical;;;#total = 1584; 
 acc = 65.34%; f1_male = 74.37% (p: 59.98 / r: 97.85); f1_female = 48.92% (p: 95.94 / r: 32.83)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 81.57%, female: 17.11%, neutral: 0.0%, unknown: 1.33%
male professions = ['developer', 'mechanic', 'mover', 'analyst', 'chief', 'salesperson', 'lawyer', 'physician', 'manager', 'laborer', 'construction worker', 'carpenter', 'janitor', 'accountant', 'supervisor', 'sheriff']
female professions = ['secretary']
neutral professions = []
ambiguous professions = ['designer', 'clerk', 'housekeeper', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'cleaner', 'cashier', 'tailor', 'writer', 'counselor', 'attendant']
Unknown male: 6
Unknown female: 15
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fce5537da60>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fce5537d700>,
                                           {<GENDER.male: 0>: 775,
                                            <GENDER.female: 1>: 11,
                                            <GENDER.unknown: 3>: 6}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fce55142b80>,
                                             {<GENDER.male: 0>: 517,
                                              <GENDER.female: 1>: 260,
                                              <GENDER.unknown: 3>: 15})})
{"acc": 65.3, "f1_male": 74.4, "f1_female": 48.9, "unk_male": 6, "unk_female": 15, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 43.62%; f1_male = 58.9% (p: 46.75 / r: 79.6); f1_female = 11.78% (p: 27.83 / r: 7.47)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 85.35%, female: 13.38%, neutral: 0.0%, unknown: 1.26%
male professions = ['salesperson', 'hairdresser', 'manager', 'cleaner', 'laborer', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'sheriff']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'librarian', 'lawyer', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cashier', 'tailor', 'supervisor', 'attendant']
Unknown male: 9
Unknown female: 11
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f90219430d0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9021943f70>,
                                           {<GENDER.male: 0>: 632,
                                            <GENDER.female: 1>: 153,
                                            <GENDER.unknown: 3>: 9}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9021943a60>,
                                             {<GENDER.male: 0>: 720,
                                              <GENDER.female: 1>: 59,
                                              <GENDER.unknown: 3>: 11})})
{"acc": 43.6, "f1_male": 58.9, "f1_female": 11.8, "unk_male": 9, "unk_female": 11, "unk_neutral": 0}
model_b
all;;;#total = 3888; 
 acc = 68.75%; f1_male = 73.99% (p: 62.83 / r: 89.98); f1_female = 67.76% (p: 84.56 / r: 56.53)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 67.26%, female: 31.33%, neutral: 0.0%, unknown: 1.41%
male professions = ['sheriff', 'technician', 'employee', 'administrator', 'onlooker', 'specialist', 'electrician', 'officer', 'plumber', 'surgeon', 'owner', 'examiner', 'dispatcher', 'bystander', 'guest', 'chef', 'doctor', 'firefighter']
female professions = ['child', 'witness', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'customer', 'someone', 'taxpayer', 'engineer', 'client', 'pedestrian', 'worker', 'student', 'educator', 'patient', 'homeowner', 'inspector', 'therapist', 'teenager', 'undergraduate', 'visitor', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'investigator', 'bartender', 'protester', 'pathologist', 'resident', 'planner', 'practitioner', 'instructor', 'veterinarian', 'paramedic', 'passenger', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'architect', 'programmer', 'paralegal', 'hygienist', 'scientist', 'dietitian', 'painter', 'broker']
Unknown male: 20
Unknown female: 33
Unknown neutral: 2
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f81b2574dc0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f81b1c8dd30>,
                                           {<GENDER.male: 0>: 1643,
                                            <GENDER.female: 1>: 163,
                                            <GENDER.unknown: 3>: 20}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f81b1c8d160>,
                                             {<GENDER.male: 0>: 759,
                                              <GENDER.female: 1>: 1030,
                                              <GENDER.unknown: 3>: 33}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f81b1c8d1f0>,
                                              {<GENDER.male: 0>: 213,
                                               <GENDER.female: 1>: 25,
                                               <GENDER.unknown: 3>: 2})})
{"acc": 68.8, "f1_male": 74.0, "f1_female": 67.8, "unk_male": 20, "unk_female": 33, "unk_neutral": 2}
pro-stereotypical;;;#total = 1584; 
 acc = 81.94%; f1_male = 85.13% (p: 77.07 / r: 95.08); f1_female = 79.73% (p: 94.78 / r: 68.81)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 61.68%, female: 36.3%, neutral: 0.0%, unknown: 2.02%
male professions = ['developer', 'mechanic', 'chief', 'salesperson', 'lawyer', 'physician', 'farmer', 'manager', 'laborer', 'carpenter', 'janitor', 'sheriff']
female professions = ['cleaner']
neutral professions = []
ambiguous professions = ['designer', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'accountant', 'supervisor', 'attendant']
Unknown male: 9
Unknown female: 23
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f89f14b2ee0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f89f14b2f70>,
                                           {<GENDER.male: 0>: 753,
                                            <GENDER.female: 1>: 30,
                                            <GENDER.unknown: 3>: 9}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f89f10769d0>,
                                             {<GENDER.male: 0>: 224,
                                              <GENDER.female: 1>: 545,
                                              <GENDER.unknown: 3>: 23})})
{"acc": 81.9, "f1_male": 85.2, "f1_female": 79.7, "unk_male": 9, "unk_female": 23, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 65.85%; f1_male = 71.44% (p: 62.43 / r: 83.5); f1_female = 59.01% (p: 76.31 / r: 48.1)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 67.05%, female: 31.44%, neutral: 0.0%, unknown: 1.52%
male professions = ['librarian', 'hairdresser', 'teacher', 'editor', 'counselor', 'accountant', 'sheriff']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'carpenter', 'janitor', 'supervisor', 'attendant']
Unknown male: 13
Unknown female: 11
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fbe33038c10>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fbe33038ee0>,
                                           {<GENDER.male: 0>: 663,
                                            <GENDER.female: 1>: 118,
                                            <GENDER.unknown: 3>: 13}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fbe330381f0>,
                                             {<GENDER.male: 0>: 399,
                                              <GENDER.female: 1>: 380,
                                              <GENDER.unknown: 3>: 11})})
{"acc": 65.8, "f1_male": 71.4, "f1_female": 59.0, "unk_male": 13, "unk_female": 11, "unk_neutral": 0}
model_cbs
all;;;aws
all;;;aws
all;;;------------>  [0, 1]
aws
all;;;------------>  [0, 1]
aws
all;;;#total = 3888; 
 acc = 65.69%; f1_male = 71.57% (p: 59.85 / r: 88.99); f1_female = 62.75% (p: 81.56 / r: 50.99)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 69.83%, female: 29.3%, neutral: 0.0%, unknown: 0.87%
male professions = ['taxpayer', 'pedestrian', 'inspector', 'therapist', 'onlooker', 'specialist', 'electrician', 'officer', 'pathologist', 'plumber', 'surgeon', 'examiner', 'buyer', 'paralegal', 'dispatcher', 'bystander', 'doctor', 'firefighter']
female professions = ['child', 'witness', 'bartender', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'employee', 'engineer', 'client', 'worker', 'student', 'educator', 'patient', 'homeowner', 'teenager', 'undergraduate', 'administrator', 'visitor', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'investigator', 'protester', 'resident', 'planner', 'practitioner', 'instructor', 'owner', 'veterinarian', 'paramedic', 'passenger', 'chemist', 'machinist', 'appraiser', 'nutritionist', 'architect', 'programmer', 'hygienist', 'scientist', 'dietitian', 'painter', 'broker', 'guest', 'chef']
Unknown male: 13
Unknown female: 19
Unknown neutral: 2
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f8abc595670>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f8abc5958b0>,
                                           {<GENDER.male: 0>: 1625,
                                            <GENDER.female: 1>: 188,
                                            <GENDER.unknown: 3>: 13}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f8abc5959d0>,
                                             {<GENDER.male: 0>: 874,
                                              <GENDER.female: 1>: 929,
                                              <GENDER.unknown: 3>: 19}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f8abc595af0>,
                                              {<GENDER.male: 0>: 216,
                                               <GENDER.female: 1>: 22,
                                               <GENDER.unknown: 3>: 2})})
{"acc": 65.7, "f1_male": 71.6, "f1_female": 62.8, "unk_male": 13, "unk_female": 19, "unk_neutral": 2}
pro-stereotypical;;;#total = 1584; 
 acc = 81.12%; f1_male = 84.33% (p: 73.88 / r: 98.23); f1_female = 77.41% (p: 97.88 / r: 64.02)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 66.48%, female: 32.7%, neutral: 0.0%, unknown: 0.82%
male professions = ['developer', 'mechanic', 'analyst', 'chief', 'salesperson', 'lawyer', 'physician', 'farmer', 'manager', 'laborer', 'construction worker', 'carpenter', 'janitor', 'supervisor', 'sheriff']
female professions = ['nurse', 'cleaner']
neutral professions = []
ambiguous professions = ['designer', 'clerk', 'mover', 'housekeeper', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'ceo', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cashier', 'tailor', 'writer', 'counselor', 'accountant', 'attendant']
Unknown male: 3
Unknown female: 10
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f97b2b89f70>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f97bf00e040>,
                                           {<GENDER.male: 0>: 778,
                                            <GENDER.female: 1>: 11,
                                            <GENDER.unknown: 3>: 3}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f97bf00eb80>,
                                             {<GENDER.male: 0>: 275,
                                              <GENDER.female: 1>: 507,
                                              <GENDER.unknown: 3>: 10})})
{"acc": 81.1, "f1_male": 84.3, "f1_female": 77.4, "unk_male": 3, "unk_female": 10, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 58.96%; f1_male = 65.89% (p: 56.86 / r: 78.34); f1_female = 49.29% (p: 65.55 / r: 39.49)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 69.07%, female: 30.05%, neutral: 0.0%, unknown: 0.88%
male professions = ['clerk', 'assistant', 'librarian', 'hairdresser', 'teacher', 'editor', 'writer', 'accountant', 'attendant']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'mover', 'housekeeper', 'analyst', 'chief', 'salesperson', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'construction worker', 'counselor', 'carpenter', 'janitor', 'supervisor', 'sheriff']
Unknown male: 8
Unknown female: 6
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fbfe538a160>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fbfe538a0d0>,
                                           {<GENDER.male: 0>: 622,
                                            <GENDER.female: 1>: 164,
                                            <GENDER.unknown: 3>: 8}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fbfe538a9d0>,
                                             {<GENDER.male: 0>: 472,
                                              <GENDER.female: 1>: 312,
                                              <GENDER.unknown: 3>: 6})})
{"acc": 59.0, "f1_male": 65.9, "f1_female": 49.3, "unk_male": 8, "unk_female": 6, "unk_neutral": 0}
bing
all;;;#total = 3888; 
 acc = 57.69%; f1_male = 67.51% (p: 53.83 / r: 90.53); f1_female = 45.57% (p: 76.92 / r: 32.38)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 78.99%, female: 19.73%, neutral: 0.0%, unknown: 1.29%
male professions = ['physician', 'janitor', 'sheriff', 'technician', 'taxpayer', 'employee', 'pedestrian', 'inspector', 'therapist', 'visitor', 'pharmacist', 'onlooker', 'investigator', 'specialist', 'electrician', 'officer', 'protester', 'pathologist', 'plumber', 'surgeon', 'owner', 'veterinarian', 'paramedic', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'scientist', 'dispatcher', 'bystander', 'painter', 'guest', 'doctor', 'firefighter']
female professions = ['child', 'advisee', 'witness', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'accountant', 'supervisor', 'attendant', 'customer', 'someone', 'engineer', 'client', 'worker', 'student', 'educator', 'patient', 'homeowner', 'teenager', 'undergraduate', 'administrator', 'advisor', 'psychologist', 'bartender', 'resident', 'planner', 'practitioner', 'instructor', 'passenger', 'nutritionist', 'architect', 'programmer', 'paralegal', 'hygienist', 'dietitian', 'broker', 'chef']
Unknown male: 14
Unknown female: 35
Unknown neutral: 1
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7faea3d26040>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7faea3d269d0>,
                                           {<GENDER.male: 0>: 1653,
                                            <GENDER.female: 1>: 159,
                                            <GENDER.unknown: 3>: 14}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7faea3d26670>,
                                             {<GENDER.male: 0>: 1197,
                                              <GENDER.female: 1>: 590,
                                              <GENDER.unknown: 3>: 35}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7faea3d26dc0>,
                                              {<GENDER.male: 0>: 221,
                                               <GENDER.female: 1>: 18,
                                               <GENDER.unknown: 3>: 1})})
{"acc": 57.7, "f1_male": 67.5, "f1_female": 45.6, "unk_male": 14, "unk_female": 35, "unk_neutral": 1}
pro-stereotypical;;;#total = 1584; 
 acc = 72.1%; f1_male = 78.42% (p: 65.37 / r: 97.98); f1_female = 62.67% (p: 97.34 / r: 46.21)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 74.94%, female: 23.74%, neutral: 0.0%, unknown: 1.33%
male professions = ['developer', 'analyst', 'chief', 'salesperson', 'lawyer', 'physician', 'farmer', 'manager', 'laborer', 'carpenter', 'janitor', 'supervisor', 'sheriff']
female professions = ['secretary']
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'cleaner', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'accountant', 'attendant']
Unknown male: 6
Unknown female: 15
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fc9c337ff70>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fc9c3383f70>,
                                           {<GENDER.male: 0>: 776,
                                            <GENDER.female: 1>: 10,
                                            <GENDER.unknown: 3>: 6}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fc9c3383310>,
                                             {<GENDER.male: 0>: 411,
                                              <GENDER.female: 1>: 366,
                                              <GENDER.unknown: 3>: 15})})
{"acc": 72.1, "f1_male": 78.4, "f1_female": 62.7, "unk_male": 6, "unk_female": 15, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 50.13%; f1_male = 62.55% (p: 50.75 / r: 81.49); f1_female = 27.33% (p: 51.4 / r: 18.61)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 80.49%, female: 18.06%, neutral: 0.0%, unknown: 1.45%
male professions = ['assistant', 'librarian', 'hairdresser', 'teacher', 'physician', 'writer', 'counselor', 'carpenter', 'janitor', 'accountant', 'attendant', 'sheriff']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'chief', 'salesperson', 'lawyer', 'cook', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'construction worker', 'supervisor']
Unknown male: 8
Unknown female: 15
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fe172924c10>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe172924820>,
                                           {<GENDER.male: 0>: 647,
                                            <GENDER.female: 1>: 139,
                                            <GENDER.unknown: 3>: 8}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe172924430>,
                                             {<GENDER.male: 0>: 628,
                                              <GENDER.female: 1>: 147,
                                              <GENDER.unknown: 3>: 15})})
{"acc": 50.1, "f1_male": 62.5, "f1_female": 27.3, "unk_male": 8, "unk_female": 15, "unk_neutral": 0}
google
all;;;#total = 3888; 
 acc = 50.98%; f1_male = 63.75% (p: 49.6 / r: 89.21); f1_female = 29.72% (p: 63.83 / r: 19.37)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 84.47%, female: 14.22%, neutral: 0.0%, unknown: 1.31%
male professions = ['salesperson', 'manager', 'laborer', 'construction worker', 'janitor', 'accountant', 'sheriff', 'customer', 'technician', 'taxpayer', 'employee', 'engineer', 'client', 'pedestrian', 'homeowner', 'inspector', 'therapist', 'administrator', 'visitor', 'advisee', 'advisor', 'pharmacist', 'onlooker', 'investigator', 'specialist', 'electrician', 'officer', 'protester', 'pathologist', 'planner', 'practitioner', 'plumber', 'instructor', 'surgeon', 'owner', 'veterinarian', 'paramedic', 'passenger', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'architect', 'programmer', 'paralegal', 'scientist', 'dispatcher', 'bystander', 'dietitian', 'painter', 'broker', 'guest', 'chef', 'doctor', 'firefighter']
female professions = ['undergraduate', 'child', 'witness', 'bartender', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'counselor', 'carpenter', 'supervisor', 'attendant', 'someone', 'worker', 'student', 'educator', 'patient', 'teenager', 'psychologist', 'resident', 'nutritionist', 'hygienist']
Unknown male: 18
Unknown female: 30
Unknown neutral: 3
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fbf12d01dc0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fbf12cbf3a0>,
                                           {<GENDER.male: 0>: 1629,
                                            <GENDER.female: 1>: 179,
                                            <GENDER.unknown: 3>: 18}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fbf12cbfaf0>,
                                             {<GENDER.male: 0>: 1439,
                                              <GENDER.female: 1>: 353,
                                              <GENDER.unknown: 3>: 30}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fbf12d013a0>,
                                              {<GENDER.male: 0>: 216,
                                               <GENDER.female: 1>: 21,
                                               <GENDER.unknown: 3>: 3})})
{"acc": 51.0, "f1_male": 63.8, "f1_female": 29.8, "unk_male": 18, "unk_female": 30, "unk_neutral": 3}
pro-stereotypical;;;#total = 1584; 
 acc = 65.34%; f1_male = 74.37% (p: 59.98 / r: 97.85); f1_female = 48.92% (p: 95.94 / r: 32.83)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 81.57%, female: 17.11%, neutral: 0.0%, unknown: 1.33%
male professions = ['developer', 'mechanic', 'mover', 'analyst', 'chief', 'salesperson', 'lawyer', 'physician', 'manager', 'laborer', 'construction worker', 'carpenter', 'janitor', 'accountant', 'supervisor', 'sheriff']
female professions = ['secretary']
neutral professions = []
ambiguous professions = ['designer', 'clerk', 'housekeeper', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'cleaner', 'cashier', 'tailor', 'writer', 'counselor', 'attendant']
Unknown male: 6
Unknown female: 15
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f80bdcb3a60>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f80bdcb3700>,
                                           {<GENDER.male: 0>: 775,
                                            <GENDER.female: 1>: 11,
                                            <GENDER.unknown: 3>: 6}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f80bd943b80>,
                                             {<GENDER.male: 0>: 517,
                                              <GENDER.female: 1>: 260,
                                              <GENDER.unknown: 3>: 15})})
{"acc": 65.3, "f1_male": 74.4, "f1_female": 48.9, "unk_male": 6, "unk_female": 15, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 43.62%; f1_male = 58.9% (p: 46.75 / r: 79.6); f1_female = 11.78% (p: 27.83 / r: 7.47)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 85.35%, female: 13.38%, neutral: 0.0%, unknown: 1.26%
male professions = ['salesperson', 'hairdresser', 'manager', 'cleaner', 'laborer', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'sheriff']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'librarian', 'lawyer', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cashier', 'tailor', 'supervisor', 'attendant']
Unknown male: 9
Unknown female: 11
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fdacc9420d0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fdacc942f70>,
                                           {<GENDER.male: 0>: 632,
                                            <GENDER.female: 1>: 153,
                                            <GENDER.unknown: 3>: 9}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fdacc942a60>,
                                             {<GENDER.male: 0>: 720,
                                              <GENDER.female: 1>: 59,
                                              <GENDER.unknown: 3>: 11})})
{"acc": 43.6, "f1_male": 58.9, "f1_female": 11.8, "unk_male": 9, "unk_female": 11, "unk_neutral": 0}
model_b
all;;;#total = 3888; 
 acc = 68.75%; f1_male = 73.99% (p: 62.83 / r: 89.98); f1_female = 67.76% (p: 84.56 / r: 56.53)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 67.26%, female: 31.33%, neutral: 0.0%, unknown: 1.41%
male professions = ['sheriff', 'technician', 'employee', 'administrator', 'onlooker', 'specialist', 'electrician', 'officer', 'plumber', 'surgeon', 'owner', 'examiner', 'dispatcher', 'bystander', 'guest', 'chef', 'doctor', 'firefighter']
female professions = ['child', 'witness', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'customer', 'someone', 'taxpayer', 'engineer', 'client', 'pedestrian', 'worker', 'student', 'educator', 'patient', 'homeowner', 'inspector', 'therapist', 'teenager', 'undergraduate', 'visitor', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'investigator', 'bartender', 'protester', 'pathologist', 'resident', 'planner', 'practitioner', 'instructor', 'veterinarian', 'paramedic', 'passenger', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'architect', 'programmer', 'paralegal', 'hygienist', 'scientist', 'dietitian', 'painter', 'broker']
Unknown male: 20
Unknown female: 33
Unknown neutral: 2
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fe927474dc0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe926c83c10>,
                                           {<GENDER.male: 0>: 1643,
                                            <GENDER.female: 1>: 163,
                                            <GENDER.unknown: 3>: 20}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe926c83f70>,
                                             {<GENDER.male: 0>: 759,
                                              <GENDER.female: 1>: 1030,
                                              <GENDER.unknown: 3>: 33}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe926c83e50>,
                                              {<GENDER.male: 0>: 213,
                                               <GENDER.female: 1>: 25,
                                               <GENDER.unknown: 3>: 2})})
{"acc": 68.8, "f1_male": 74.0, "f1_female": 67.8, "unk_male": 20, "unk_female": 33, "unk_neutral": 2}
pro-stereotypical;;;#total = 1584; 
 acc = 81.94%; f1_male = 85.13% (p: 77.07 / r: 95.08); f1_female = 79.73% (p: 94.78 / r: 68.81)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 61.68%, female: 36.3%, neutral: 0.0%, unknown: 2.02%
male professions = ['developer', 'mechanic', 'chief', 'salesperson', 'lawyer', 'physician', 'farmer', 'manager', 'laborer', 'carpenter', 'janitor', 'sheriff']
female professions = ['cleaner']
neutral professions = []
ambiguous professions = ['designer', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'accountant', 'supervisor', 'attendant']
Unknown male: 9
Unknown female: 23
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fc84937cee0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fc84937cf70>,
                                           {<GENDER.male: 0>: 753,
                                            <GENDER.female: 1>: 30,
                                            <GENDER.unknown: 3>: 9}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fc8490769d0>,
                                             {<GENDER.male: 0>: 224,
                                              <GENDER.female: 1>: 545,
                                              <GENDER.unknown: 3>: 23})})
{"acc": 81.9, "f1_male": 85.2, "f1_female": 79.7, "unk_male": 9, "unk_female": 23, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 65.85%; f1_male = 71.44% (p: 62.43 / r: 83.5); f1_female = 59.01% (p: 76.31 / r: 48.1)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 67.05%, female: 31.44%, neutral: 0.0%, unknown: 1.52%
male professions = ['librarian', 'hairdresser', 'teacher', 'editor', 'counselor', 'accountant', 'sheriff']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'carpenter', 'janitor', 'supervisor', 'attendant']
Unknown male: 13
Unknown female: 11
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fc631038c10>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fc631038ee0>,
                                           {<GENDER.male: 0>: 663,
                                            <GENDER.female: 1>: 118,
                                            <GENDER.unknown: 3>: 13}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fc6310381f0>,
                                             {<GENDER.male: 0>: 399,
                                              <GENDER.female: 1>: 380,
                                              <GENDER.unknown: 3>: 11})})
{"acc": 65.8, "f1_male": 71.4, "f1_female": 59.0, "unk_male": 13, "unk_female": 11, "unk_neutral": 0}
model_cbs
all;;;#total = 3888; 
 acc = 70.09%; f1_male = 75.44% (p: 65.15 / r: 89.59); f1_female = 70.19% (p: 85.01 / r: 59.77)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 64.58%, female: 32.95%, neutral: 0.0%, unknown: 2.47%
male professions = ['technician', 'employee', 'pedestrian', 'administrator', 'visitor', 'onlooker', 'specialist', 'officer', 'pathologist', 'plumber', 'surgeon', 'examiner', 'dispatcher', 'bystander', 'guest', 'doctor', 'firefighter']
female professions = ['witness']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'taxpayer', 'engineer', 'client', 'worker', 'student', 'educator', 'patient', 'homeowner', 'inspector', 'therapist', 'teenager', 'undergraduate', 'child', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'investigator', 'bartender', 'electrician', 'protester', 'victim', 'resident', 'planner', 'practitioner', 'instructor', 'owner', 'veterinarian', 'paramedic', 'passenger', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'architect', 'programmer', 'paralegal', 'hygienist', 'scientist', 'dietitian', 'painter', 'broker', 'chef']
Unknown male: 25
Unknown female: 59
Unknown neutral: 12
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7faca64b85e0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7faca6d64dc0>,
                                           {<GENDER.male: 0>: 1636,
                                            <GENDER.female: 1>: 165,
                                            <GENDER.unknown: 3>: 25}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7faca6d649d0>,
                                             {<GENDER.male: 0>: 674,
                                              <GENDER.female: 1>: 1089,
                                              <GENDER.unknown: 3>: 59}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7faca6d64f70>,
                                              {<GENDER.male: 0>: 201,
                                               <GENDER.female: 1>: 27,
                                               <GENDER.unknown: 3>: 12})})
{"acc": 70.1, "f1_male": 75.5, "f1_female": 70.2, "unk_male": 25, "unk_female": 59, "unk_neutral": 12}
pro-stereotypical;;;#total = 1584; 
 acc = 80.68%; f1_male = 84.44% (p: 77.2 / r: 93.18); f1_female = 78.37% (p: 92.15 / r: 68.18)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 60.35%, female: 36.99%, neutral: 0.0%, unknown: 2.65%
male professions = ['developer', 'mechanic', 'chief', 'salesperson', 'lawyer', 'physician', 'farmer', 'manager', 'laborer', 'carpenter', 'janitor', 'sheriff']
female professions = ['secretary', 'cleaner']
neutral professions = []
ambiguous professions = ['designer', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'accountant', 'supervisor', 'attendant']
Unknown male: 8
Unknown female: 34
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fdaf1375f70>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fdaf1377310>,
                                           {<GENDER.male: 0>: 738,
                                            <GENDER.female: 1>: 46,
                                            <GENDER.unknown: 3>: 8}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fdaf1377c10>,
                                             {<GENDER.male: 0>: 218,
                                              <GENDER.female: 1>: 540,
                                              <GENDER.unknown: 3>: 34})})
{"acc": 80.7, "f1_male": 84.4, "f1_female": 78.4, "unk_male": 8, "unk_female": 34, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 71.59%; f1_male = 76.41% (p: 68.57 / r: 86.27); f1_female = 67.37% (p: 82.69 / r: 56.84)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 63.07%, female: 34.28%, neutral: 0.0%, unknown: 2.65%
male professions = ['hairdresser', 'teacher', 'editor']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 15
Unknown female: 27
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fd42204bdc0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd42204bf70>,
                                           {<GENDER.male: 0>: 685,
                                            <GENDER.female: 1>: 94,
                                            <GENDER.unknown: 3>: 15}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd42204b940>,
                                             {<GENDER.male: 0>: 314,
                                              <GENDER.female: 1>: 449,
                                              <GENDER.unknown: 3>: 27})})
{"acc": 71.6, "f1_male": 76.4, "f1_female": 67.3, "unk_male": 15, "unk_female": 27, "unk_neutral": 0}
padrao-mbart-finetuned-news_commentary
all;;;#total = 3888; 
 acc = 54.35%; f1_male = 66.64% (p: 52.71 / r: 90.58); f1_female = 38.28% (p: 79.69 / r: 25.19)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 80.71%, female: 14.81%, neutral: 0.0%, unknown: 4.48%
male professions = ['physician', 'customer', 'technician', 'taxpayer', 'employee', 'engineer', 'client', 'pedestrian', 'worker', 'student', 'educator', 'patient', 'inspector', 'teenager', 'undergraduate', 'administrator', 'visitor', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'onlooker', 'investigator', 'specialist', 'electrician', 'officer', 'protester', 'pathologist', 'resident', 'planner', 'practitioner', 'plumber', 'instructor', 'surgeon', 'owner', 'veterinarian', 'paramedic', 'passenger', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'architect', 'programmer', 'hygienist', 'scientist', 'dispatcher', 'bystander', 'dietitian', 'painter', 'broker', 'guest', 'chef', 'doctor', 'firefighter']
female professions = ['nurse', 'witness', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'someone', 'homeowner', 'therapist', 'child', 'bartender', 'paralegal']
Unknown male: 65
Unknown female: 105
Unknown neutral: 4
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fcbcdc55160>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fcbcdc55940>,
                                           {<GENDER.male: 0>: 1654,
                                            <GENDER.female: 1>: 107,
                                            <GENDER.unknown: 3>: 65}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fcbcdc55310>,
                                             {<GENDER.male: 0>: 1258,
                                              <GENDER.female: 1>: 459,
                                              <GENDER.unknown: 3>: 105}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fcbcdc55670>,
                                              {<GENDER.male: 0>: 226,
                                               <GENDER.female: 1>: 10,
                                               <GENDER.unknown: 3>: 4})})
{"acc": 54.3, "f1_male": 66.6, "f1_female": 38.3, "unk_male": 65, "unk_female": 105, "unk_neutral": 4}
pro-stereotypical;;;#total = 1584; 
 acc = 68.69%; f1_male = 77.99% (p: 65.69 / r: 95.96); f1_female = 57.84% (p: 95.91 / r: 41.41)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 73.04%, female: 21.59%, neutral: 0.0%, unknown: 5.37%
male professions = ['analyst', 'salesperson', 'lawyer', 'physician', 'laborer', 'supervisor', 'sheriff']
female professions = ['nurse']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'assistant', 'chief', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'attendant']
Unknown male: 18
Unknown female: 67
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fb86d13d040>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fb86d13d160>,
                                           {<GENDER.male: 0>: 760,
                                            <GENDER.female: 1>: 14,
                                            <GENDER.unknown: 3>: 18}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fb86d13db80>,
                                             {<GENDER.male: 0>: 397,
                                              <GENDER.female: 1>: 328,
                                              <GENDER.unknown: 3>: 67})})
{"acc": 68.7, "f1_male": 78.0, "f1_female": 57.8, "unk_male": 18, "unk_female": 67, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 49.81%; f1_male = 63.39% (p: 50.76 / r: 84.38); f1_female = 23.82% (p: 56.94 / r: 15.06)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 83.33%, female: 13.19%, neutral: 0.0%, unknown: 3.47%
male professions = ['hairdresser', 'teacher', 'physician', 'secretary']
female professions = ['nurse']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'cook', 'baker', 'farmer', 'ceo', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 34
Unknown female: 21
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f97c8083430>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f97c8083310>,
                                           {<GENDER.male: 0>: 670,
                                            <GENDER.female: 1>: 90,
                                            <GENDER.unknown: 3>: 34}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f97c80830d0>,
                                             {<GENDER.male: 0>: 650,
                                              <GENDER.female: 1>: 119,
                                              <GENDER.unknown: 3>: 21})})
{"acc": 49.8, "f1_male": 63.4, "f1_female": 23.9, "unk_male": 34, "unk_female": 21, "unk_neutral": 0}
padrao-mbart-finetuned-opus_books
all;;;#total = 3888; 
 acc = 52.34%; f1_male = 64.5% (p: 51.24 / r: 87.02); f1_female = 36.29% (p: 70.13 / r: 24.48)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 79.76%, female: 16.36%, neutral: 0.0%, unknown: 3.88%
male professions = ['developer', 'physician', 'customer', 'technician', 'taxpayer', 'employee', 'engineer', 'client', 'pedestrian', 'worker', 'student', 'educator', 'patient', 'homeowner', 'inspector', 'administrator', 'visitor', 'advisor', 'pharmacist', 'onlooker', 'investigator', 'bartender', 'specialist', 'electrician', 'officer', 'protester', 'pathologist', 'resident', 'planner', 'practitioner', 'plumber', 'instructor', 'surgeon', 'owner', 'veterinarian', 'passenger', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'architect', 'programmer', 'paralegal', 'hygienist', 'dispatcher', 'bystander', 'dietitian', 'painter', 'broker', 'guest', 'chef', 'doctor', 'firefighter']
female professions = ['child', 'witness', 'victim']
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'someone', 'therapist', 'teenager', 'undergraduate', 'advisee', 'psychologist', 'paramedic', 'nutritionist', 'scientist']
Unknown male: 62
Unknown female: 82
Unknown neutral: 7
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f9101f5cd30>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f90f5c62790>,
                                           {<GENDER.male: 0>: 1589,
                                            <GENDER.female: 1>: 175,
                                            <GENDER.unknown: 3>: 62}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f90f5c62940>,
                                             {<GENDER.male: 0>: 1294,
                                              <GENDER.female: 1>: 446,
                                              <GENDER.unknown: 3>: 82}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f90f5c628b0>,
                                              {<GENDER.male: 0>: 218,
                                               <GENDER.female: 1>: 15,
                                               <GENDER.unknown: 3>: 7})})
{"acc": 52.3, "f1_male": 64.5, "f1_female": 36.3, "unk_male": 62, "unk_female": 82, "unk_neutral": 7}
pro-stereotypical;;;#total = 1584; 
 acc = 66.04%; f1_male = 75.28% (p: 63.03 / r: 93.43); f1_female = 53.88% (p: 88.95 / r: 38.64)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 74.12%, female: 21.72%, neutral: 0.0%, unknown: 4.17%
male professions = ['developer', 'analyst', 'chief', 'lawyer', 'physician', 'manager', 'supervisor', 'sheriff']
female professions = ['secretary']
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'assistant', 'salesperson', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'attendant']
Unknown male: 14
Unknown female: 52
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fbc223c0280>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fbc08c7c160>,
                                           {<GENDER.male: 0>: 740,
                                            <GENDER.female: 1>: 38,
                                            <GENDER.unknown: 3>: 14}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fbc08c7cdc0>,
                                             {<GENDER.male: 0>: 434,
                                              <GENDER.female: 1>: 306,
                                              <GENDER.unknown: 3>: 52})})
{"acc": 66.0, "f1_male": 75.2, "f1_female": 53.8, "unk_male": 14, "unk_female": 52, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 47.22%; f1_male = 60.56% (p: 49.06 / r: 79.09); f1_female = 23.03% (p: 47.62 / r: 15.19)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 80.81%, female: 15.91%, neutral: 0.0%, unknown: 3.28%
male professions = ['developer', 'designer', 'physician', 'writer', 'counselor', 'janitor', 'sheriff']
female professions = []
neutral professions = []
ambiguous professions = ['mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'construction worker', 'carpenter', 'accountant', 'supervisor', 'attendant']
Unknown male: 34
Unknown female: 18
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fcf32f6a4c0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fcf32f6ac10>,
                                           {<GENDER.male: 0>: 628,
                                            <GENDER.female: 1>: 132,
                                            <GENDER.unknown: 3>: 34}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fcf32f6adc0>,
                                             {<GENDER.male: 0>: 652,
                                              <GENDER.female: 1>: 120,
                                              <GENDER.unknown: 3>: 18})})
{"acc": 47.2, "f1_male": 60.6, "f1_female": 23.0, "unk_male": 34, "unk_female": 18, "unk_neutral": 0}
padrao-unicamp-finetuned-news_commentary
all;;;#total = 3888; 
 acc = 61.63%; f1_male = 70.59% (p: 57.76 / r: 90.74); f1_female = 54.68% (p: 83.88 / r: 40.56)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 73.79%, female: 22.66%, neutral: 0.0%, unknown: 3.55%
male professions = ['mover', 'taxpayer', 'employee', 'client', 'pedestrian', 'homeowner', 'inspector', 'visitor', 'advisee', 'advisor', 'investigator', 'electrician', 'officer', 'protester', 'pathologist', 'resident', 'planner', 'plumber', 'instructor', 'surgeon', 'owner', 'veterinarian', 'paramedic', 'passenger', 'examiner', 'buyer', 'architect', 'paralegal', 'bystander', 'painter', 'guest']
female professions = ['therapist', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'engineer', 'worker', 'student', 'educator', 'patient', 'teenager', 'undergraduate', 'administrator', 'child', 'pharmacist', 'psychologist', 'onlooker', 'witness', 'bartender', 'specialist', 'practitioner', 'chemist', 'machinist', 'appraiser', 'nutritionist', 'programmer', 'hygienist', 'scientist', 'dispatcher', 'dietitian', 'broker', 'chef', 'doctor', 'firefighter']
Unknown male: 46
Unknown female: 63
Unknown neutral: 29
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7ff780342790>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7ff77fc35b80>,
                                           {<GENDER.male: 0>: 1657,
                                            <GENDER.female: 1>: 123,
                                            <GENDER.unknown: 3>: 46}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7ff7803423a0>,
                                             {<GENDER.male: 0>: 1020,
                                              <GENDER.female: 1>: 739,
                                              <GENDER.unknown: 3>: 63}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7ff7803428b0>,
                                              {<GENDER.male: 0>: 192,
                                               <GENDER.female: 1>: 19,
                                               <GENDER.unknown: 3>: 29})})
{"acc": 61.6, "f1_male": 70.6, "f1_female": 54.7, "unk_male": 46, "unk_female": 63, "unk_neutral": 29}
pro-stereotypical;;;#total = 1584; 
 acc = 63.07%; f1_male = 71.24% (p: 59.61 / r: 88.51); f1_female = 50.77% (p: 78.01 / r: 37.63)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 74.24%, female: 24.12%, neutral: 0.0%, unknown: 1.64%
male professions = ['developer', 'mover', 'analyst', 'salesperson', 'lawyer', 'physician', 'driver', 'laborer', 'construction worker', 'carpenter', 'supervisor']
female professions = []
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'housekeeper', 'assistant', 'chief', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'counselor', 'janitor', 'accountant', 'attendant', 'sheriff']
Unknown male: 7
Unknown female: 19
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f9ee1b77670>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9ee1b77550>,
                                           {<GENDER.male: 0>: 701,
                                            <GENDER.female: 1>: 84,
                                            <GENDER.unknown: 3>: 7}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9ee1bd0b80>,
                                             {<GENDER.male: 0>: 475,
                                              <GENDER.female: 1>: 298,
                                              <GENDER.unknown: 3>: 19})})
{"acc": 63.1, "f1_male": 71.2, "f1_female": 50.7, "unk_male": 7, "unk_female": 19, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 70.14%; f1_male = 76.58% (p: 64.27 / r: 94.71); f1_female = 61.0% (p: 92.76 / r: 45.44)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 73.86%, female: 24.43%, neutral: 0.0%, unknown: 1.7%
male professions = ['mover', 'librarian', 'hairdresser', 'teacher', 'editor']
female professions = ['mechanic']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'clerk', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 14
Unknown female: 13
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7feaec5acf70>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7feaec5ae280>,
                                           {<GENDER.male: 0>: 752,
                                            <GENDER.female: 1>: 28,
                                            <GENDER.unknown: 3>: 14}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7feaec5ae550>,
                                             {<GENDER.male: 0>: 418,
                                              <GENDER.female: 1>: 359,
                                              <GENDER.unknown: 3>: 13})})
{"acc": 70.1, "f1_male": 76.6, "f1_female": 61.0, "unk_male": 14, "unk_female": 13, "unk_neutral": 0}
padrao-unicamp-finetuned-opus_books
all;;;#total = 3888; 
 acc = 63.76%; f1_male = 73.05% (p: 62.58 / r: 87.73); f1_female = 61.45% (p: 84.98 / r: 48.13)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 65.84%, female: 26.54%, neutral: 0.0%, unknown: 7.61%
male professions = ['taxpayer', 'employee', 'pedestrian', 'homeowner', 'inspector', 'advisee', 'electrician', 'protester', 'pathologist', 'resident', 'planner', 'surgeon', 'veterinarian', 'paramedic', 'passenger', 'paralegal', 'painter', 'guest', 'chef', 'firefighter']
female professions = ['victim', 'dietitian']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'engineer', 'client', 'worker', 'student', 'educator', 'patient', 'therapist', 'teenager', 'undergraduate', 'administrator', 'visitor', 'child', 'advisor', 'pharmacist', 'psychologist', 'onlooker', 'witness', 'investigator', 'bartender', 'specialist', 'officer', 'practitioner', 'plumber', 'instructor', 'owner', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'architect', 'programmer', 'hygienist', 'scientist', 'dispatcher', 'bystander', 'broker', 'doctor']
Unknown male: 93
Unknown female: 157
Unknown neutral: 46
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f9a9ad8dca0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9a9ad8dc10>,
                                           {<GENDER.male: 0>: 1602,
                                            <GENDER.female: 1>: 131,
                                            <GENDER.unknown: 3>: 93}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9a9ad8dd30>,
                                             {<GENDER.male: 0>: 788,
                                              <GENDER.female: 1>: 877,
                                              <GENDER.unknown: 3>: 157}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9a9ad8da60>,
                                              {<GENDER.male: 0>: 170,
                                               <GENDER.female: 1>: 24,
                                               <GENDER.unknown: 3>: 46})})
{"acc": 63.8, "f1_male": 73.1, "f1_female": 61.4, "unk_male": 93, "unk_female": 157, "unk_neutral": 46}
pro-stereotypical;;;#total = 1584; 
 acc = 67.49%; f1_male = 75.2% (p: 65.72 / r: 87.88); f1_female = 59.92% (p: 82.34 / r: 47.1)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 66.86%, female: 28.6%, neutral: 0.0%, unknown: 4.55%
male professions = ['mover', 'lawyer', 'manager', 'driver', 'laborer']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 16
Unknown female: 56
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fb8cf884820>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fb8cf8849d0>,
                                           {<GENDER.male: 0>: 696,
                                            <GENDER.female: 1>: 80,
                                            <GENDER.unknown: 3>: 16}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fb8cf884e50>,
                                             {<GENDER.male: 0>: 363,
                                              <GENDER.female: 1>: 373,
                                              <GENDER.unknown: 3>: 56})})
{"acc": 67.5, "f1_male": 75.2, "f1_female": 59.9, "unk_male": 16, "unk_female": 56, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 72.79%; f1_male = 78.8% (p: 69.45 / r: 91.06); f1_female = 67.88% (p: 90.15 / r: 54.43)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 65.72%, female: 30.11%, neutral: 0.0%, unknown: 4.17%
male professions = ['hairdresser', 'teacher']
female professions = ['mechanic']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 24
Unknown female: 42
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fcc21384dc0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fcc210e38b0>,
                                           {<GENDER.male: 0>: 723,
                                            <GENDER.female: 1>: 47,
                                            <GENDER.unknown: 3>: 24}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fcc213a30d0>,
                                             {<GENDER.male: 0>: 318,
                                              <GENDER.female: 1>: 430,
                                              <GENDER.unknown: 3>: 42})})
{"acc": 72.8, "f1_male": 78.8, "f1_female": 67.8, "unk_male": 24, "unk_female": 42, "unk_neutral": 0}
padrao-unicamp-vanessa-finetuned-handscrafted
all;;;#total = 3888; 
 acc = 71.48%; f1_male = 77.0% (p: 67.17 / r: 90.2); f1_female = 72.91% (p: 88.23 / r: 62.13)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 63.07%, female: 33.0%, neutral: 0.0%, unknown: 3.94%
male professions = ['employee', 'client', 'pedestrian', 'homeowner', 'inspector', 'protester', 'pathologist', 'resident', 'planner', 'surgeon', 'veterinarian', 'passenger', 'paralegal', 'guest', 'firefighter']
female professions = ['therapist', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'taxpayer', 'engineer', 'worker', 'student', 'educator', 'patient', 'teenager', 'undergraduate', 'administrator', 'visitor', 'child', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'onlooker', 'witness', 'investigator', 'bartender', 'specialist', 'electrician', 'officer', 'practitioner', 'plumber', 'instructor', 'owner', 'paramedic', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'architect', 'programmer', 'hygienist', 'scientist', 'dispatcher', 'bystander', 'dietitian', 'painter', 'broker', 'chef', 'doctor']
Unknown male: 50
Unknown female: 77
Unknown neutral: 26
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f861a424e50>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f861ac593a0>,
                                           {<GENDER.male: 0>: 1647,
                                            <GENDER.female: 1>: 129,
                                            <GENDER.unknown: 3>: 50}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f861ac599d0>,
                                             {<GENDER.male: 0>: 613,
                                              <GENDER.female: 1>: 1132,
                                              <GENDER.unknown: 3>: 77}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f861a424a60>,
                                              {<GENDER.male: 0>: 192,
                                               <GENDER.female: 1>: 22,
                                               <GENDER.unknown: 3>: 26})})
{"acc": 71.5, "f1_male": 77.0, "f1_female": 72.9, "unk_male": 50, "unk_female": 77, "unk_neutral": 26}
pro-stereotypical;;;#total = 1584; 
 acc = 72.73%; f1_male = 77.71% (p: 69.41 / r: 88.26); f1_female = 68.17% (p: 84.36 / r: 57.2)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 63.57%, female: 33.9%, neutral: 0.0%, unknown: 2.53%
male professions = ['developer', 'mover', 'analyst', 'salesperson', 'lawyer', 'physician', 'driver', 'laborer', 'construction worker', 'supervisor']
female professions = []
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'housekeeper', 'assistant', 'chief', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'counselor', 'carpenter', 'janitor', 'accountant', 'attendant', 'sheriff']
Unknown male: 9
Unknown female: 31
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f8d432b43a0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f8d432b4a60>,
                                           {<GENDER.male: 0>: 699,
                                            <GENDER.female: 1>: 84,
                                            <GENDER.unknown: 3>: 9}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f8d432b4e50>,
                                             {<GENDER.male: 0>: 308,
                                              <GENDER.female: 1>: 453,
                                              <GENDER.unknown: 3>: 31})})
{"acc": 72.7, "f1_male": 77.7, "f1_female": 68.2, "unk_male": 9, "unk_female": 31, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 81.94%; f1_male = 85.25% (p: 77.84 / r: 94.21); f1_female = 80.17% (p: 94.5 / r: 69.62)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 60.67%, female: 36.74%, neutral: 0.0%, unknown: 2.59%
male professions = ['librarian', 'hairdresser', 'teacher', 'editor', 'secretary']
female professions = ['mechanic']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 14
Unknown female: 27
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7ff008b75700>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7ff008b75550>,
                                           {<GENDER.male: 0>: 748,
                                            <GENDER.female: 1>: 32,
                                            <GENDER.unknown: 3>: 14}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7ff008b754c0>,
                                             {<GENDER.male: 0>: 213,
                                              <GENDER.female: 1>: 550,
                                              <GENDER.unknown: 3>: 27})})
{"acc": 81.9, "f1_male": 85.2, "f1_female": 80.2, "unk_male": 14, "unk_female": 27, "unk_neutral": 0}
pt-unicamp-handcrafted
all;;;#total = 3888; 
 acc = 72.69%; f1_male = 77.65% (p: 68.07 / r: 90.36); f1_female = 74.43% (p: 87.89 / r: 64.54)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 62.35%, female: 34.41%, neutral: 0.0%, unknown: 3.24%
male professions = ['employee', 'engineer', 'client', 'pedestrian', 'homeowner', 'inspector', 'specialist', 'protester', 'pathologist', 'resident', 'planner', 'plumber', 'surgeon', 'veterinarian', 'passenger', 'architect', 'paralegal', 'guest', 'firefighter']
female professions = ['victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'taxpayer', 'worker', 'student', 'educator', 'patient', 'therapist', 'teenager', 'undergraduate', 'administrator', 'visitor', 'child', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'onlooker', 'witness', 'investigator', 'bartender', 'electrician', 'officer', 'practitioner', 'instructor', 'owner', 'paramedic', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'programmer', 'hygienist', 'scientist', 'dispatcher', 'bystander', 'dietitian', 'painter', 'broker', 'chef', 'doctor']
Unknown male: 37
Unknown female: 77
Unknown neutral: 12
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fda4755a940>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fda46da9af0>,
                                           {<GENDER.male: 0>: 1650,
                                            <GENDER.female: 1>: 139,
                                            <GENDER.unknown: 3>: 37}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fda4755a8b0>,
                                             {<GENDER.male: 0>: 569,
                                              <GENDER.female: 1>: 1176,
                                              <GENDER.unknown: 3>: 77}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fda4755ad30>,
                                              {<GENDER.male: 0>: 205,
                                               <GENDER.female: 1>: 23,
                                               <GENDER.unknown: 3>: 12})})
{"acc": 72.7, "f1_male": 77.7, "f1_female": 74.4, "unk_male": 37, "unk_female": 77, "unk_neutral": 12}
pro-stereotypical;;;#total = 1584; 
 acc = 74.56%; f1_male = 78.8% (p: 71.34 / r: 88.01); f1_female = 71.07% (p: 84.91 / r: 61.11)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 61.68%, female: 35.98%, neutral: 0.0%, unknown: 2.34%
male professions = ['developer', 'mover', 'analyst', 'salesperson', 'lawyer', 'physician', 'driver', 'laborer', 'construction worker', 'carpenter', 'supervisor', 'sheriff']
female professions = []
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'housekeeper', 'assistant', 'chief', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'counselor', 'janitor', 'accountant', 'attendant']
Unknown male: 9
Unknown female: 28
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fad8483f040>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fad8483fdc0>,
                                           {<GENDER.male: 0>: 697,
                                            <GENDER.female: 1>: 86,
                                            <GENDER.unknown: 3>: 9}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fad8483f820>,
                                             {<GENDER.male: 0>: 280,
                                              <GENDER.female: 1>: 484,
                                              <GENDER.unknown: 3>: 28})})
{"acc": 74.6, "f1_male": 78.8, "f1_female": 71.1, "unk_male": 9, "unk_female": 28, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 82.32%; f1_male = 85.6% (p: 79.14 / r: 93.2); f1_female = 80.8% (p: 93.07 / r: 71.39)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 59.03%, female: 38.26%, neutral: 0.0%, unknown: 2.71%
male professions = ['hairdresser', 'teacher', 'editor', 'secretary']
female professions = ['mechanic']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 12
Unknown female: 31
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fa6b1001ee0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa6b1004280>,
                                           {<GENDER.male: 0>: 740,
                                            <GENDER.female: 1>: 42,
                                            <GENDER.unknown: 3>: 12}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa6b1004040>,
                                             {<GENDER.male: 0>: 195,
                                              <GENDER.female: 1>: 564,
                                              <GENDER.unknown: 3>: 31})})
{"acc": 82.3, "f1_male": 85.6, "f1_female": 80.8, "unk_male": 12, "unk_female": 31, "unk_neutral": 0}
pt-unicamp-handcrafted-puro
all;;;#total = 3888; 
 acc = 72.99%; f1_male = 79.48% (p: 71.55 / r: 89.38); f1_female = 75.54% (p: 87.96 / r: 66.19)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 58.67%, female: 35.26%, neutral: 0.0%, unknown: 6.07%
male professions = ['employee', 'client', 'pedestrian', 'inspector', 'bartender', 'specialist', 'electrician', 'protester', 'pathologist', 'resident', 'plumber', 'surgeon', 'veterinarian', 'passenger', 'paralegal', 'painter', 'guest']
female professions = ['therapist', 'undergraduate', 'witness', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'taxpayer', 'engineer', 'worker', 'student', 'educator', 'patient', 'homeowner', 'teenager', 'administrator', 'visitor', 'child', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'onlooker', 'investigator', 'officer', 'planner', 'practitioner', 'instructor', 'owner', 'paramedic', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'architect', 'programmer', 'hygienist', 'scientist', 'dispatcher', 'bystander', 'dietitian', 'broker', 'chef', 'doctor', 'firefighter']
Unknown male: 67
Unknown female: 135
Unknown neutral: 34
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f854b58a310>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f854b590820>,
                                           {<GENDER.male: 0>: 1632,
                                            <GENDER.female: 1>: 127,
                                            <GENDER.unknown: 3>: 67}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f854b590790>,
                                             {<GENDER.male: 0>: 481,
                                              <GENDER.female: 1>: 1206,
                                              <GENDER.unknown: 3>: 135}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f854b58ab80>,
                                              {<GENDER.male: 0>: 168,
                                               <GENDER.female: 1>: 38,
                                               <GENDER.unknown: 3>: 34})})
{"acc": 73.0, "f1_male": 79.5, "f1_female": 75.6, "unk_male": 67, "unk_female": 135, "unk_neutral": 34}
pro-stereotypical;;;#total = 1584; 
 acc = 75.44%; f1_male = 81.06% (p: 74.32 / r: 89.14); f1_female = 72.07% (p: 86.55 / r: 61.74)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 59.97%, female: 35.67%, neutral: 0.0%, unknown: 4.36%
male professions = ['developer', 'mover', 'chief', 'salesperson', 'lawyer', 'physician', 'manager', 'laborer', 'supervisor']
female professions = []
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'housekeeper', 'analyst', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'attendant', 'sheriff']
Unknown male: 10
Unknown female: 59
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fccbf36ff70>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fccbf3713a0>,
                                           {<GENDER.male: 0>: 706,
                                            <GENDER.female: 1>: 76,
                                            <GENDER.unknown: 3>: 10}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fccbf07fa60>,
                                             {<GENDER.male: 0>: 244,
                                              <GENDER.female: 1>: 489,
                                              <GENDER.unknown: 3>: 59})})
{"acc": 75.4, "f1_male": 81.0, "f1_female": 72.0, "unk_male": 10, "unk_female": 59, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 83.21%; f1_male = 87.0% (p: 81.0 / r: 93.95); f1_female = 82.19% (p: 95.02 / r: 72.41)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 58.14%, female: 38.01%, neutral: 0.0%, unknown: 3.85%
male professions = ['receptionist', 'writer']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 18
Unknown female: 43
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fdbc96e6ee0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fdbc96e8820>,
                                           {<GENDER.male: 0>: 746,
                                            <GENDER.female: 1>: 30,
                                            <GENDER.unknown: 3>: 18}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fdbc96e8310>,
                                             {<GENDER.male: 0>: 175,
                                              <GENDER.female: 1>: 572,
                                              <GENDER.unknown: 3>: 43})})
{"acc": 83.2, "f1_male": 87.0, "f1_female": 82.2, "unk_male": 18, "unk_female": 43, "unk_neutral": 0}
systran
all;;;#total = 3888; 
 acc = 60.19%; f1_male = 68.99% (p: 55.53 / r: 91.07); f1_female = 50.91% (p: 80.79 / r: 37.16)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 77.03%, female: 21.55%, neutral: 0.0%, unknown: 1.41%
male professions = ['mover', 'sheriff', 'technician', 'taxpayer', 'engineer', 'client', 'pedestrian', 'inspector', 'advisee', 'onlooker', 'specialist', 'electrician', 'officer', 'pathologist', 'planner', 'plumber', 'instructor', 'surgeon', 'veterinarian', 'paramedic', 'passenger', 'examiner', 'buyer', 'architect', 'dispatcher', 'bystander', 'dietitian', 'chef', 'doctor', 'firefighter']
female professions = ['therapist', 'undergraduate', 'child', 'witness', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'customer', 'someone', 'employee', 'worker', 'student', 'educator', 'patient', 'homeowner', 'teenager', 'administrator', 'visitor', 'advisor', 'pharmacist', 'psychologist', 'investigator', 'bartender', 'protester', 'resident', 'practitioner', 'owner', 'chemist', 'machinist', 'appraiser', 'nutritionist', 'programmer', 'paralegal', 'hygienist', 'scientist', 'painter', 'broker', 'guest']
Unknown male: 22
Unknown female: 30
Unknown neutral: 3
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f963ed87e50>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f963e5a5820>,
                                           {<GENDER.male: 0>: 1663,
                                            <GENDER.female: 1>: 141,
                                            <GENDER.unknown: 3>: 22}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f963ed87b80>,
                                             {<GENDER.male: 0>: 1115,
                                              <GENDER.female: 1>: 677,
                                              <GENDER.unknown: 3>: 30}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f963e46e160>,
                                              {<GENDER.male: 0>: 217,
                                               <GENDER.female: 1>: 20,
                                               <GENDER.unknown: 3>: 3})})
{"acc": 60.2, "f1_male": 69.0, "f1_female": 50.9, "unk_male": 22, "unk_female": 30, "unk_neutral": 3}
pro-stereotypical;;;#total = 1584; 
 acc = 73.61%; f1_male = 79.39% (p: 66.84 / r: 97.73); f1_female = 65.55% (p: 97.03 / r: 49.49)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 73.11%, female: 25.51%, neutral: 0.0%, unknown: 1.39%
male professions = ['developer', 'mover', 'analyst', 'chief', 'salesperson', 'lawyer', 'physician', 'manager', 'laborer', 'construction worker', 'carpenter', 'supervisor', 'sheriff']
female professions = ['secretary']
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'housekeeper', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'cleaner', 'cashier', 'tailor', 'writer', 'counselor', 'janitor', 'accountant', 'attendant']
Unknown male: 6
Unknown female: 16
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fa093821700>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa0938219d0>,
                                           {<GENDER.male: 0>: 774,
                                            <GENDER.female: 1>: 12,
                                            <GENDER.unknown: 3>: 6}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa093821430>,
                                             {<GENDER.male: 0>: 384,
                                              <GENDER.female: 1>: 392,
                                              <GENDER.unknown: 3>: 16})})
{"acc": 73.6, "f1_male": 79.3, "f1_female": 65.5, "unk_male": 6, "unk_female": 16, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 54.36%; f1_male = 65.32% (p: 53.49 / r: 83.88); f1_female = 35.42% (p: 62.7 / r: 24.68)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 78.6%, female: 19.63%, neutral: 0.0%, unknown: 1.77%
male professions = ['mover', 'assistant', 'salesperson', 'hairdresser', 'teacher', 'carpenter', 'sheriff']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'housekeeper', 'analyst', 'chief', 'librarian', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'janitor', 'accountant', 'supervisor', 'attendant']
Unknown male: 12
Unknown female: 16
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fd19b947c10>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd19b947a60>,
                                           {<GENDER.male: 0>: 666,
                                            <GENDER.female: 1>: 116,
                                            <GENDER.unknown: 3>: 12}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd19b9475e0>,
                                             {<GENDER.male: 0>: 579,
                                              <GENDER.female: 1>: 195,
                                              <GENDER.unknown: 3>: 16})})
{"acc": 54.4, "f1_male": 65.3, "f1_female": 35.4, "unk_male": 12, "unk_female": 16, "unk_neutral": 0}
unicamp-dl-translation-en-pt-t5
all;;;#total = 3888; 
 acc = 68.42%; f1_male = 76.25% (p: 66.37 / r: 89.59); f1_female = 68.4% (p: 87.37 / r: 56.2)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 63.4%, female: 30.14%, neutral: 0.0%, unknown: 6.46%
male professions = ['taxpayer', 'employee', 'client', 'pedestrian', 'homeowner', 'inspector', 'bartender', 'specialist', 'protester', 'pathologist', 'resident', 'planner', 'plumber', 'surgeon', 'owner', 'veterinarian', 'passenger', 'paralegal', 'painter', 'guest']
female professions = ['therapist', 'witness', 'victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'engineer', 'worker', 'student', 'educator', 'patient', 'teenager', 'undergraduate', 'administrator', 'visitor', 'child', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'onlooker', 'investigator', 'electrician', 'officer', 'practitioner', 'instructor', 'paramedic', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'architect', 'programmer', 'hygienist', 'scientist', 'dispatcher', 'bystander', 'dietitian', 'broker', 'chef', 'doctor', 'firefighter']
Unknown male: 77
Unknown female: 141
Unknown neutral: 33
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f9527656b80>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9526e738b0>,
                                           {<GENDER.male: 0>: 1636,
                                            <GENDER.female: 1>: 113,
                                            <GENDER.unknown: 3>: 77}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9527656ee0>,
                                             {<GENDER.male: 0>: 657,
                                              <GENDER.female: 1>: 1024,
                                              <GENDER.unknown: 3>: 141}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9526d43670>,
                                              {<GENDER.male: 0>: 172,
                                               <GENDER.female: 1>: 35,
                                               <GENDER.unknown: 3>: 33})})
{"acc": 68.4, "f1_male": 76.3, "f1_female": 68.4, "unk_male": 77, "unk_female": 141, "unk_neutral": 33}
pro-stereotypical;;;#total = 1584; 
 acc = 70.2%; f1_male = 77.62% (p: 68.88 / r: 88.89); f1_female = 64.05% (p: 84.65 / r: 51.52)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 64.52%, female: 30.43%, neutral: 0.0%, unknown: 5.05%
male professions = ['mover', 'chief', 'salesperson', 'lawyer', 'physician', 'laborer', 'supervisor']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'housekeeper', 'analyst', 'assistant', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'attendant', 'sheriff']
Unknown male: 14
Unknown female: 66
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fe03d940430>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe03d940310>,
                                           {<GENDER.male: 0>: 704,
                                            <GENDER.female: 1>: 74,
                                            <GENDER.unknown: 3>: 14}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fe03d9403a0>,
                                             {<GENDER.male: 0>: 318,
                                              <GENDER.female: 1>: 408,
                                              <GENDER.unknown: 3>: 66})})
{"acc": 70.2, "f1_male": 77.6, "f1_female": 64.0, "unk_male": 14, "unk_female": 66, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 77.84%; f1_male = 82.93% (p: 74.23 / r: 93.95); f1_female = 74.7% (p: 94.75 / r: 61.65)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 63.45%, female: 32.45%, neutral: 0.0%, unknown: 4.1%
male professions = ['hairdresser', 'receptionist', 'secretary', 'writer']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'guard', 'editor', 'cleaner', 'laborer', 'cashier', 'tailor', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 21
Unknown female: 44
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f9c21ca8af0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9c218e5dc0>,
                                           {<GENDER.male: 0>: 746,
                                            <GENDER.female: 1>: 27,
                                            <GENDER.unknown: 3>: 21}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9c21ca8dc0>,
                                             {<GENDER.male: 0>: 259,
                                              <GENDER.female: 1>: 487,
                                              <GENDER.unknown: 3>: 44})})
{"acc": 77.8, "f1_male": 82.9, "f1_female": 74.6, "unk_male": 21, "unk_female": 44, "unk_neutral": 0}
unicamp-finetuned-en-to-pt-dataset-ted
all;;;#total = 3888; 
 acc = 44.6%; f1_male = 60.79% (p: 47.8 / r: 83.46); f1_female = 18.81% (p: 51.09 / r: 11.53)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 82.0%, female: 10.57%, neutral: 0.0%, unknown: 7.43%
male professions = ['manager', 'carpenter', 'supervisor', 'customer', 'technician', 'taxpayer', 'employee', 'engineer', 'client', 'worker', 'educator', 'homeowner', 'inspector', 'therapist', 'teenager', 'visitor', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'onlooker', 'investigator', 'specialist', 'electrician', 'officer', 'protester', 'pathologist', 'victim', 'resident', 'planner', 'plumber', 'instructor', 'surgeon', 'owner', 'paramedic', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'architect', 'programmer', 'paralegal', 'hygienist', 'scientist', 'dispatcher', 'bystander', 'painter', 'broker', 'guest', 'chef', 'doctor', 'firefighter']
female professions = ['undergraduate', 'child', 'practitioner', 'veterinarian', 'passenger', 'nutritionist']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'janitor', 'accountant', 'attendant', 'sheriff', 'someone', 'pedestrian', 'student', 'patient', 'administrator', 'witness', 'bartender', 'dietitian']
Unknown male: 123
Unknown female: 140
Unknown neutral: 26
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fd7aec86670>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd7ae4c6160>,
                                           {<GENDER.male: 0>: 1524,
                                            <GENDER.female: 1>: 179,
                                            <GENDER.unknown: 3>: 123}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd7aec86700>,
                                             {<GENDER.male: 0>: 1472,
                                              <GENDER.female: 1>: 210,
                                              <GENDER.unknown: 3>: 140}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd7aec86f70>,
                                              {<GENDER.male: 0>: 192,
                                               <GENDER.female: 1>: 22,
                                               <GENDER.unknown: 3>: 26})})
{"acc": 44.6, "f1_male": 60.8, "f1_female": 18.8, "unk_male": 123, "unk_female": 140, "unk_neutral": 26}
pro-stereotypical;;;#total = 1584; 
 acc = 49.56%; f1_male = 64.43% (p: 51.43 / r: 86.24); f1_female = 21.14% (p: 58.96 / r: 12.88)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 83.84%, female: 10.92%, neutral: 0.0%, unknown: 5.24%
male professions = ['manager', 'editor', 'carpenter', 'supervisor']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'janitor', 'accountant', 'attendant', 'sheriff']
Unknown male: 38
Unknown female: 45
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fc677387550>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fc6773870d0>,
                                           {<GENDER.male: 0>: 683,
                                            <GENDER.female: 1>: 71,
                                            <GENDER.unknown: 3>: 38}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fc6773874c0>,
                                             {<GENDER.male: 0>: 645,
                                              <GENDER.female: 1>: 102,
                                              <GENDER.unknown: 3>: 45})})
{"acc": 49.6, "f1_male": 64.4, "f1_female": 21.2, "unk_male": 38, "unk_female": 45, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 47.66%; f1_male = 62.8% (p: 50.23 / r: 83.75); f1_female = 18.52% (p: 49.45 / r: 11.39)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 83.59%, female: 11.49%, neutral: 0.0%, unknown: 4.92%
male professions = ['manager', 'carpenter', 'supervisor']
female professions = []
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'janitor', 'accountant', 'attendant', 'sheriff']
Unknown male: 37
Unknown female: 41
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f9f1fb83b80>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9f1f8889d0>,
                                           {<GENDER.male: 0>: 665,
                                            <GENDER.female: 1>: 92,
                                            <GENDER.unknown: 3>: 37}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f9f1fb83ee0>,
                                             {<GENDER.male: 0>: 659,
                                              <GENDER.female: 1>: 90,
                                              <GENDER.unknown: 3>: 41})})
{"acc": 47.7, "f1_male": 62.8, "f1_female": 18.5, "unk_male": 37, "unk_female": 41, "unk_neutral": 0}
unicamp_handcrafted
all;;;#total = 3888; 
 acc = 73.05%; f1_male = 77.94% (p: 68.4 / r: 90.58); f1_female = 75.06% (p: 88.64 / r: 65.09)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 62.19%, female: 34.41%, neutral: 0.0%, unknown: 3.4%
male professions = ['employee', 'engineer', 'client', 'pedestrian', 'homeowner', 'inspector', 'bartender', 'specialist', 'protester', 'pathologist', 'resident', 'planner', 'surgeon', 'veterinarian', 'passenger', 'paralegal', 'guest', 'firefighter']
female professions = ['victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'taxpayer', 'worker', 'student', 'educator', 'patient', 'therapist', 'teenager', 'undergraduate', 'administrator', 'visitor', 'child', 'advisee', 'advisor', 'pharmacist', 'psychologist', 'onlooker', 'witness', 'investigator', 'electrician', 'officer', 'practitioner', 'plumber', 'instructor', 'owner', 'paramedic', 'examiner', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'architect', 'programmer', 'hygienist', 'scientist', 'dispatcher', 'bystander', 'dietitian', 'painter', 'broker', 'chef', 'doctor']
Unknown male: 40
Unknown female: 79
Unknown neutral: 13
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fea05b3bb80>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fea055b85e0>,
                                           {<GENDER.male: 0>: 1654,
                                            <GENDER.female: 1>: 132,
                                            <GENDER.unknown: 3>: 40}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fea05b3bdc0>,
                                             {<GENDER.male: 0>: 557,
                                              <GENDER.female: 1>: 1186,
                                              <GENDER.unknown: 3>: 79}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fea05b3bf70>,
                                              {<GENDER.male: 0>: 207,
                                               <GENDER.female: 1>: 20,
                                               <GENDER.unknown: 3>: 13})})
{"acc": 73.0, "f1_male": 78.0, "f1_female": 75.1, "unk_male": 40, "unk_female": 79, "unk_neutral": 13}
pro-stereotypical;;;#total = 1584; 
 acc = 74.43%; f1_male = 78.94% (p: 71.56 / r: 88.01); f1_female = 70.94% (p: 85.01 / r: 60.86)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 61.49%, female: 35.8%, neutral: 0.0%, unknown: 2.71%
male professions = ['developer', 'mover', 'analyst', 'salesperson', 'lawyer', 'physician', 'driver', 'laborer', 'construction worker', 'supervisor', 'sheriff']
female professions = []
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'housekeeper', 'assistant', 'chief', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'counselor', 'carpenter', 'janitor', 'accountant', 'attendant']
Unknown male: 10
Unknown female: 33
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f7fa18c11f0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f7fa18c1820>,
                                           {<GENDER.male: 0>: 697,
                                            <GENDER.female: 1>: 85,
                                            <GENDER.unknown: 3>: 10}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f7fa18c1b80>,
                                             {<GENDER.male: 0>: 277,
                                              <GENDER.female: 1>: 482,
                                              <GENDER.unknown: 3>: 33})})
{"acc": 74.4, "f1_male": 79.0, "f1_female": 71.0, "unk_male": 10, "unk_female": 33, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 83.46%; f1_male = 86.26% (p: 79.91 / r: 93.7); f1_female = 82.33% (p: 94.14 / r: 73.16)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 58.78%, female: 38.76%, neutral: 0.0%, unknown: 2.46%
male professions = ['librarian', 'hairdresser', 'teacher', 'editor', 'secretary']
female professions = ['mechanic']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'clerk', 'mover', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 14
Unknown female: 25
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fee6292c5e0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fee6292ce50>,
                                           {<GENDER.male: 0>: 744,
                                            <GENDER.female: 1>: 36,
                                            <GENDER.unknown: 3>: 14}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fee6292cee0>,
                                             {<GENDER.male: 0>: 187,
                                              <GENDER.female: 1>: 578,
                                              <GENDER.unknown: 3>: 25})})
{"acc": 83.5, "f1_male": 86.3, "f1_female": 82.3, "unk_male": 14, "unk_female": 25, "unk_neutral": 0}
unicamp_news
all;;;#total = 3888; 
 acc = 64.02%; f1_male = 72.0% (p: 59.37 / r: 91.46); f1_female = 58.86% (p: 85.22 / r: 44.95)
Gold distribution: male: 46.97% (1826), female: 46.86% (1822), neutral: 6.17% (240), unknown: 0.0% (0)
Predictions: male: 72.35%, female: 24.72%, neutral: 0.0%, unknown: 2.93%
male professions = ['mover', 'taxpayer', 'employee', 'client', 'pedestrian', 'homeowner', 'inspector', 'advisee', 'advisor', 'bartender', 'specialist', 'electrician', 'officer', 'protester', 'pathologist', 'resident', 'planner', 'plumber', 'surgeon', 'owner', 'veterinarian', 'passenger', 'examiner', 'architect', 'paralegal', 'bystander', 'painter', 'guest']
female professions = ['victim']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'mechanic', 'clerk', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'librarian', 'lawyer', 'hairdresser', 'cook', 'teacher', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'counselor', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff', 'customer', 'someone', 'technician', 'engineer', 'worker', 'student', 'educator', 'patient', 'therapist', 'teenager', 'undergraduate', 'administrator', 'visitor', 'child', 'pharmacist', 'psychologist', 'onlooker', 'witness', 'investigator', 'practitioner', 'instructor', 'paramedic', 'chemist', 'machinist', 'buyer', 'appraiser', 'nutritionist', 'programmer', 'hygienist', 'scientist', 'dispatcher', 'dietitian', 'broker', 'chef', 'doctor', 'firefighter']
Unknown male: 34
Unknown female: 66
Unknown neutral: 14
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fa776c5be50>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa7763f9af0>,
                                           {<GENDER.male: 0>: 1670,
                                            <GENDER.female: 1>: 122,
                                            <GENDER.unknown: 3>: 34}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa7763f90d0>,
                                             {<GENDER.male: 0>: 937,
                                              <GENDER.female: 1>: 819,
                                              <GENDER.unknown: 3>: 66}),
             <GENDER.neutral: 2>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fa7763f9f70>,
                                              {<GENDER.male: 0>: 206,
                                               <GENDER.female: 1>: 20,
                                               <GENDER.unknown: 3>: 14})})
{"acc": 64.0, "f1_male": 72.0, "f1_female": 58.9, "unk_male": 34, "unk_female": 66, "unk_neutral": 14}
pro-stereotypical;;;#total = 1584; 
 acc = 64.65%; f1_male = 72.45% (p: 61.33 / r: 88.51); f1_female = 53.88% (p: 79.36 / r: 40.78)
Gold distribution: male: 50.0% (792), female: 50.0% (792), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 72.16%, female: 25.69%, neutral: 0.0%, unknown: 2.15%
male professions = ['developer', 'mover', 'analyst', 'salesperson', 'lawyer', 'physician', 'driver', 'laborer', 'construction worker', 'supervisor']
female professions = []
neutral professions = []
ambiguous professions = ['designer', 'mechanic', 'clerk', 'housekeeper', 'assistant', 'chief', 'librarian', 'hairdresser', 'cook', 'teacher', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'auditor', 'receptionist', 'guard', 'editor', 'secretary', 'cleaner', 'cashier', 'tailor', 'writer', 'counselor', 'carpenter', 'janitor', 'accountant', 'attendant', 'sheriff']
Unknown male: 7
Unknown female: 27
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7fd613373f70>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd61ef849d0>,
                                           {<GENDER.male: 0>: 701,
                                            <GENDER.female: 1>: 84,
                                            <GENDER.unknown: 3>: 7}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7fd61ef84ee0>,
                                             {<GENDER.male: 0>: 442,
                                              <GENDER.female: 1>: 323,
                                              <GENDER.unknown: 3>: 27})})
{"acc": 64.6, "f1_male": 72.4, "f1_female": 53.9, "unk_male": 7, "unk_female": 27, "unk_neutral": 0}
anti-stereotypical;;;#total = 1584; 
 acc = 72.47%; f1_male = 78.11% (p: 66.52 / r: 94.58); f1_female = 65.29% (p: 93.19 / r: 50.25)
Gold distribution: male: 50.13% (794), female: 49.87% (790), neutral: 0.0% (0), unknown: 0.0% (0)
Predictions: male: 71.28%, female: 26.89%, neutral: 0.0%, unknown: 1.83%
male professions = ['mover', 'librarian', 'hairdresser', 'teacher', 'editor', 'counselor']
female professions = ['mechanic']
neutral professions = []
ambiguous professions = ['developer', 'designer', 'clerk', 'housekeeper', 'analyst', 'assistant', 'chief', 'salesperson', 'lawyer', 'cook', 'physician', 'baker', 'farmer', 'ceo', 'nurse', 'manager', 'driver', 'auditor', 'receptionist', 'guard', 'secretary', 'cleaner', 'laborer', 'cashier', 'tailor', 'writer', 'construction worker', 'carpenter', 'janitor', 'accountant', 'supervisor', 'attendant', 'sheriff']
Unknown male: 14
Unknown female: 15
Unknown neutral: 0
defaultdict(<function evaluate_bias_complete.<locals>.<lambda> at 0x7f89124acee0>,
            {<GENDER.male: 0>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f891df91e50>,
                                           {<GENDER.male: 0>: 751,
                                            <GENDER.female: 1>: 29,
                                            <GENDER.unknown: 3>: 14}),
             <GENDER.female: 1>: defaultdict(<function evaluate_bias_complete.<locals>.<lambda>.<locals>.<lambda> at 0x7f891df91550>,
                                             {<GENDER.male: 0>: 378,
                                              <GENDER.female: 1>: 397,
                                              <GENDER.unknown: 3>: 15})})
{"acc": 72.5, "f1_male": 78.1, "f1_female": 65.3, "unk_male": 14, "unk_female": 15, "unk_neutral": 0}
